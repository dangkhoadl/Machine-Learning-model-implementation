{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "## 1.1 Problem\n",
    "- Translate **human language date** to a `machine standard date` format\n",
    "- Eg:\n",
    "    + **the 29th of August 1958** -> `1958-08-29`\n",
    "    + **03/30/1968**              -> `1968-03-30`\n",
    "    + **24 JUNE 1987**            -> `1987-06-24`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babel.dates import format_date\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# data generator\n",
    "fake = Faker()\n",
    "\n",
    "date_formats = ['short', 'medium', 'long',\n",
    "    'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full',\n",
    "    'd MMM YYY',  'd MMMM YYY', 'dd MMM YYY', 'd MMM, YYY', 'd MMMM, YYY', 'dd, MMM YYY',\n",
    "    'd MM YY', 'd MMMM YYY', 'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY']\n",
    "\n",
    "def generate_training_example():\n",
    "    # Get a random date (standard format)\n",
    "    machine_date = fake.date_object()\n",
    "    \n",
    "    # Generate a human readable format\n",
    "    human_readable = format_date(machine_date, format=random.choice(date_formats), locale='en_US') \\\n",
    "        .lower().replace(',','')\n",
    "    \n",
    "    return human_readable, machine_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "july 6 1991                    -> 1991-07-06\n",
      "26 mar 1986                    -> 1986-03-26\n",
      "march 16 2007                  -> 2007-03-16\n",
      "06 dec 1986                    -> 1986-12-06\n",
      "tuesday april 9 1991           -> 1991-04-09\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    human, machine = generate_training_example()\n",
    "    print(f'{human:30} -> {machine}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dset(m=10000):\n",
    "    X, Y = [], []\n",
    "    for i in range(m):\n",
    "        x, y = generate_training_example()\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunday march 9 1997', '27 march 1998', 'friday august 25 1995', '30 may 2015', 'wednesday october 25 2000']\n",
      "['1997-03-09', '1998-03-27', '1995-08-25', '2015-05-30', '2000-10-25']\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_dset(m=10000)\n",
    "\n",
    "X_train, Y_train = X[:8000], Y[:8000]\n",
    "X_test, Y_test = X[8000:], Y[8000:]\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lexicon size: 39\n",
      "{' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '<unk>': 35, '<pad>': 36, '<start>': 37, '<end>': 38}\n"
     ]
    }
   ],
   "source": [
    "X_chars = sorted(list(set(''.join(X_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "X_lexicon = { ch:idx for idx, ch in enumerate(X_chars) }\n",
    "X_lexicon_size = len(X_lexicon)\n",
    "\n",
    "print('X lexicon size:', X_lexicon_size)\n",
    "print(X_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y lexicon size: 15\n",
      "Y_lexicon = {'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '<unk>': 11, '<pad>': 12, '<start>': 13, '<end>': 14}\n",
      "Y_inverse_lexicon = {0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<unk>', 12: '<pad>', 13: '<start>', 14: '<end>'}\n"
     ]
    }
   ],
   "source": [
    "Y_chars = sorted(list(set(''.join(Y_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "\n",
    "Y_lexicon         = { ch:idx for idx, ch in enumerate(Y_chars) }\n",
    "Y_lexicon_size    = len(Y_lexicon)\n",
    "Y_inverse_lexicon = { idx:ch for idx, ch in enumerate(Y_chars) }\n",
    "\n",
    "\n",
    "print('Y lexicon size:', Y_lexicon_size)\n",
    "print(f'{Y_lexicon = }')\n",
    "print(f'{Y_inverse_lexicon = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def get_feat_tensor(data, lexicon, pad_length=30):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data    (list(str))         : input data list of utterances\n",
    "        lexicon (dict(char:index))  : lexicon data, categorical encoding char to int\n",
    "        pad_length (int)            : padded length of output utterance \n",
    "\n",
    "    Returns:\n",
    "        data_tensor (ndarray (m, pad_length)) : output tensor with\n",
    "            <m> training seamples\n",
    "            <pad_length> padded utterance size\n",
    "    \"\"\"\n",
    "\n",
    "    # Pipeline for each utterance\n",
    "    def get_utt_tensor(utt:str):\n",
    "        # Tokenize: char to int\n",
    "        utt_tensor = [ lexicon['<start>'] ] + \\\n",
    "            [ lexicon[ch] if lexicon.get(ch) is not None\n",
    "                else lexicon['<unk>']\n",
    "                    for ch in utt ] + \\\n",
    "            [ lexicon['<end>'] ]\n",
    "\n",
    "        # padding\n",
    "        utt_tensor = utt_tensor[:pad_length]\n",
    "        if len(utt_tensor) < pad_length:\n",
    "            utt_tensor += [lexicon['<pad>']]*(pad_length - len(utt_tensor))\n",
    "\n",
    "        return np.array(utt_tensor)\n",
    "\n",
    "    # Convert m examples\n",
    "    tensor = Parallel(n_jobs=16)(delayed(function=get_utt_tensor)(utt)\n",
    "        for utt in data)\n",
    "    return np.array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 32)\n",
      "(8000, 12)\n",
      "Y[0] = [13  2 10 10  8  0  1  4  0  1 10 14]\n"
     ]
    }
   ],
   "source": [
    "# Fixed sequence length\n",
    "Tx = 32\n",
    "Ty = 12\n",
    "\n",
    "X_train_ts = get_feat_tensor(X_train, X_lexicon, pad_length=Tx)\n",
    "Y_train_ts = get_feat_tensor(Y_train, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "X_test_ts = get_feat_tensor(X_test, X_lexicon, pad_length=Tx)\n",
    "Y_test_ts = get_feat_tensor(Y_test, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "# X_train = (m, Tx)\n",
    "print(X_train_ts.shape)\n",
    "\n",
    "# Y_train = (m, Ty)\n",
    "print(Y_train_ts.shape)\n",
    "print('Y[0] =', Y_train_ts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, X_lexicon_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.emb_dim = 64\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        # Emb\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=X_lexicon_size,\n",
    "            embedding_dim=self.emb_dim)\n",
    "\n",
    "        # enc\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "    def forward(self, X, device='cpu'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (Longtensor(m, Tx))                : Input Sequence, type=Long\n",
    "                m: batch_size,\n",
    "                Tx: sequence length \n",
    "        Returns:\n",
    "            o_enc (tensor(m, Tx, 2*hid_dim))         : Encoder output states (bi-directional)\n",
    "            h_enc (tensor(2*num_layers, m, hid_dim)) : Encoder hidden states (bi-directional)\n",
    "            c_enc (tensor(2*num_layers, m, hid_dim)) : Encoder cell states (bi-directional)\n",
    "        \"\"\"\n",
    "        # get batchsize\n",
    "        m = X.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        #    (m, Tx) -> (m, Tx, emb_dim)\n",
    "        emb = self.embedding(X)\n",
    "        emb = F.relu(emb)\n",
    "    \n",
    "        # Init h0, c0\n",
    "        #   (2, m, hid_dim)\n",
    "        h0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "        c0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "\n",
    "        # Encode\n",
    "        #   o_enc: (m, Tx, 2*hid_dim)\n",
    "        #   h_enc/c_enc: (2, m, hid_dim)\n",
    "        o_enc, (h_enc, c_enc) = self.encoder(emb, (h0, c0))\n",
    "        return o_enc, (h_enc, c_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_enc.size() = torch.Size([16, 32, 256])\n",
      "h_enc.size() = torch.Size([2, 16, 128]), c_enc.size() = torch.Size([2, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "X = torch.LongTensor(X_train_ts[:16])\n",
    "\n",
    "encoder = Encoder(X_lexicon_size=39)\n",
    "o_enc, (h_enc, c_enc) = encoder(X)\n",
    "\n",
    "print(f'{o_enc.size() = }')\n",
    "print(f'{h_enc.size() = }, {c_enc.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention - [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<img src=\"./assets/seq2seq_attention.jpg\" width=\"300\"/>\n",
    "\n",
    "- Energy $e_{ij}$\n",
    "    + $[]$: concat ops\n",
    "        + $h_j$: previous decoder hidden state\n",
    "        + $s_{i-1}$: encoder output states\n",
    "    + $f()$: attention function\n",
    "  \n",
    "$$e_{ij} = f( [h_j, s_{i-1}] )$$\n",
    "\n",
    "- Attention (weights)\n",
    "\n",
    "$$\\alpha_{ij} = \\text{softmax}(e_{ij}) = \\frac{exp(e_{ij})}{\\sum\\limits_{k=1}^{Tx}exp(e_{ik})}$$\n",
    "\n",
    "- context vector: **Tell decoder which parts of hidden state which it should pay more attention (weights) to**\n",
    "    $$c_i = \\sum\\limits_{j=1}^{Tx} \\alpha_{ij}h_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.hid_feat_dim = 128\n",
    "\n",
    "        # Aggregate hidden state\n",
    "        self.agg_hidden_fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=self.hid_feat_dim)\n",
    "\n",
    "        # Attention function\n",
    "        self.att_fc = nn.Linear(\n",
    "            in_features=3*self.hid_feat_dim,\n",
    "            out_features=1)\n",
    "\n",
    "    def forward(self, enc_states, ht_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            enc_states (tensor(m, Tx, 2*hid_dim))     : Encoder output states (bi-directional)\n",
    "            ht_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden states (bi-directional)\n",
    "        Returns:\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))  : Context vector\n",
    "        \"\"\"\n",
    "        # get dims: batch_size, sequence_size\n",
    "        m, Tx = enc_states.size(0), enc_states.size(1)\n",
    "\n",
    "        #### Aggregate\n",
    "        # Aggregate ht_dec (bi-directional to 1 dir)\n",
    "        #    (2, m, hid_dim) -> (m, 2*hid_dim) -> (m, hid_dim) \n",
    "        ht_dec_agg = self.agg_hidden_fc(\n",
    "            torch.cat([ht_dec[0], ht_dec[1]], dim=1))\n",
    "\n",
    "        #### Attention\n",
    "        # Reshape ht_dec\n",
    "        #    (m, hid_dim) -> (m, 1, hid_dim) -> (m, Tx, hid_dim) \n",
    "        ht_reshaped = torch.unsqueeze(ht_dec_agg, dim=1) \\\n",
    "            .repeat(1,Tx,1)\n",
    "\n",
    "        # Compute energy e_ij\n",
    "        #   (m, Tx, hid_dim) + (m, Tx, 2*hid_dim) -> (m, Tx, 3*hid_dim)\n",
    "        concat = torch.cat([ht_reshaped, enc_states], dim=2)\n",
    "        #   (m, Tx, 3*hid_dim) -> (m, Tx, 1)\n",
    "        energy = F.relu(self.att_fc(concat))\n",
    "\n",
    "        # Compute attention alpha_ij: (m, Tx, 1)\n",
    "        alpha = F.softmax(energy, dim=2)\n",
    "\n",
    "        # Compute context vector\n",
    "        #   (m, Tx, 1) x (m, Tx, hid_dim) -> (m, 1, 2*hid_dim)\n",
    "        context_vector = torch.einsum(\"mTk,mTn->mkn\", alpha, enc_states)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.size() = torch.Size([16, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention()\n",
    "\n",
    "context = attention(\n",
    "    enc_states=o_enc,\n",
    "    ht_dec=h_enc)\n",
    "\n",
    "print(f'{context.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, Y_lexicon_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.emb_dim = 64\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # Emb\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=Y_lexicon_size,\n",
    "            embedding_dim=self.emb_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=2*self.hid_feat_dim + self.emb_dim,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, yt_prev, context_vector, ht_dec, ct_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            yt_prev (Long tensor(m))  : Output sequence at prev time step (t-1), categorical Long\n",
    "                m: batchsize\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))          : Context vector from attention mechanism\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden/cell states (t-1) (bi-directional)\n",
    "        Returns:\n",
    "            yt_hat (tensor(m, Y_lexicon_size))                : y_hat at timestep t, out_dim = Y_lexicon_size\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : decoder hidden/cell states at current timestep t (bi-directional)\n",
    "        \"\"\"\n",
    "        # (m) -> (m, 1)\n",
    "        yt_prev = yt_prev.unsqueeze(dim=1)\n",
    "\n",
    "        # Embedding\n",
    "        #    (m, 1) -> (m, 1, emb_dim)\n",
    "        emb = self.embedding(yt_prev)\n",
    "        emb = F.relu(emb)\n",
    "\n",
    "        # Decode\n",
    "        #   (m,1,2*hid_dim) + (m,1,emb_dim) -> (m,1,2*hid_dim + emb_dim)\n",
    "        concat = torch.cat([context_vector, emb], dim=2)\n",
    "        #   o_dec:         (m, 1, 2*hid_dim)\n",
    "        #   ht_dec/ct_dec: (2, m, hid_dim)\n",
    "        o_dec, (ht_dec, ct_dec) = self.decoder(concat, (ht_dec, ct_dec))\n",
    "\n",
    "        # Predict y_hat\n",
    "        #   (m, 1, 2*hid_dim) -> (m, 1, y_lexicon_dim) -> (m, y_lexicon_dim)\n",
    "        yt_hat = self.fc(o_dec).squeeze(dim=1)\n",
    "        yt_hat = F.log_softmax(yt_hat, dim=1)\n",
    "\n",
    "        return yt_hat, ht_dec, ct_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.size() = torch.Size([16, 12]), y0.size() = torch.Size([16])\n",
      "y1_hat.size() = torch.Size([16, 15])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.LongTensor(Y_train_ts[:16])\n",
    "y0 = Y[:, 0]\n",
    "print(f'{Y.size() = }, {y0.size() = }')\n",
    "\n",
    "decoder = Decoder(Y_lexicon_size=15)\n",
    "y1_hat, ht_dec, ct_dec = decoder(yt_prev=y0,\n",
    "    context_vector=context,\n",
    "    ht_dec=h_enc, ct_dec=c_enc)\n",
    "\n",
    "print(f'{y1_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Models\n",
    "- Apply context vector to all decoder hidden states\n",
    "\n",
    "<img src=\"./assets/seq2seq_context.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, X_lexicon_size, Y_lexicon_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # Params\n",
    "        self.Y_lexicon_size = Y_lexicon_size\n",
    "\n",
    "        self.encoder = Encoder(X_lexicon_size=X_lexicon_size)\n",
    "        self.attention = Attention()\n",
    "        self.decoder = Decoder(Y_lexicon_size=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, X, Y,\n",
    "            device='cpu', teacher_force_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (Long tensor(m, Tx))        : Input sequence\n",
    "            Y (Long tensor(m, Ty))        : Output sequence\n",
    "            teacher_force_ratio (float)   : teaching forcing probability in range [0,1]\n",
    "        Returns:\n",
    "            Y_hat (tensor(m, Ty, Y_lexicon_size)): Y_hat, out_dim = Y_lexicon_size\n",
    "        \"\"\"\n",
    "        # Get dim\n",
    "        m, Ty = Y.size(0), Y.size(1)\n",
    "\n",
    "        # Encode\n",
    "        enc_out, (h_enc, c_enc) = self.encoder(X, device=device)\n",
    "\n",
    "        # Init yt_prev = <start>, long tensor (m,)\n",
    "        yt_prev = Y[:, 0]\n",
    "\n",
    "        # Init h_dec, c_dec\n",
    "        ht_dec, ct_dec = h_enc, c_enc\n",
    "\n",
    "        # Predict next timestep\n",
    "        Y_hat = torch.zeros(m, Ty, self.Y_lexicon_size).to(device)\n",
    "        for t in range(1, Ty):\n",
    "            # Attention\n",
    "            context = self.attention(\n",
    "                enc_states=enc_out,\n",
    "                ht_dec=ht_dec)\n",
    "\n",
    "            # Decode\n",
    "            Y_hat[:,t,:] , ht_dec, ct_dec = self.decoder(\n",
    "                yt_prev=yt_prev,\n",
    "                context_vector=context,\n",
    "                ht_dec=ht_dec, ct_dec=ct_dec)\n",
    "\n",
    "            # Teaching forcing:\n",
    "            #   prob:      Force yt_prev = Y[t+1]\n",
    "            #   1 - prob:  get prediction from model\n",
    "            if random.random() < teacher_force_ratio:\n",
    "                yt_prev = Y[:,t]\n",
    "            else:\n",
    "                yt_prev = Y_hat[:,t,:].argmax(dim=1)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat.size() = torch.Size([16, 12, 15])\n"
     ]
    }
   ],
   "source": [
    "net = Seq2Seq(X_lexicon_size=39, Y_lexicon_size=15)\n",
    "Y_hat = net(X, Y)\n",
    "\n",
    "print(f'{Y_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit(\n",
    "        X, Y,\n",
    "        X_lexicon_size, Y_lexicon_size, Y_pad_token,\n",
    "        alpha=1e-2, num_iters=1000, batch_size=16, teacher_force_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X (ndarray(m, Tx))      : Input sequence\n",
    "        Y (ndarray(m, Ty))      : Output sequence\n",
    "        X_lexicon_size (int)    : Input sequence feat_dim = size of vocab X.\n",
    "        Y_lexicon_size (int)    : Output sequence feat_dim = size of vocab Y.\n",
    "        Y_pad_token (int)       : Padding token of target sequence, ignore when computing cost\n",
    "    Returns:\n",
    "        net (torch model)       : trained seq2seq model\n",
    "        J_history (list)        : List of cost each iter for plotting\n",
    "    \"\"\"\n",
    "    # Dataset\n",
    "    dset = TensorDataset(\n",
    "        torch.LongTensor(X),\n",
    "        torch.LongTensor(Y))\n",
    "\n",
    "    # Dataloader\n",
    "    dloader = DataLoader(\n",
    "        dataset=dset,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    ## Config\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    net = Seq2Seq(\n",
    "        X_lexicon_size=X_lexicon_size,\n",
    "        Y_lexicon_size=Y_lexicon_size)\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    # Criterions\n",
    "    #    Note: Exclude padding token when compte loss\n",
    "    criterion = nn.NLLLoss(\n",
    "        ignore_index=Y_pad_token)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=alpha)\n",
    "\n",
    "    # cost and params history\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost = 0\n",
    "        for b, batch in enumerate(dloader):\n",
    "            # Batch:\n",
    "            #    X_b = (batch_size, Tx)\n",
    "            #    Y_b = (batch_size, Ty)\n",
    "            Xb, Yb = batch\n",
    "            Xb = Xb.to(device).to(torch.int64)\n",
    "            Yb = Yb.to(device).to(torch.int64)\n",
    "\n",
    "            # Forward\n",
    "            #    Yb_hat = (batch_size, Ty, Y_lexicon_size)\n",
    "            optimizer.zero_grad()\n",
    "            Yb_hat = net(Xb, Yb,\n",
    "                device=device, teacher_force_ratio=teacher_force_ratio)\n",
    "\n",
    "            # Batch Cost compute, t=1 skip <start>\n",
    "            Yb_hat_reshaped = Yb_hat[:,1:,:].reshape(-1, Y_lexicon_size)\n",
    "            Yb_reshaped = Yb[:,1:].reshape(-1)\n",
    "\n",
    "            cost_b = criterion(Yb_hat_reshaped, Yb_reshaped)\n",
    "\n",
    "            # Track Iter Cost\n",
    "            cost += cost_b.item()\n",
    "\n",
    "            # Back Propagation\n",
    "            cost_b.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clip grad to avoid exploding gradient\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "\n",
    "        # Compute Cost\n",
    "        J_history.append(cost)\n",
    "        if i % 10 == 0 or i == num_iters-1:\n",
    "            print(f\"Cost after iteration {i:4}: {cost:.4f}\")\n",
    "    return net, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx = 32, X_lexicon_size = 39\n",
      "Ty = 12, Y_lexicon_size = 15\n",
      "Cost after iteration    0: 68.0405\n",
      "Cost after iteration   10: 9.1068\n",
      "Cost after iteration   20: 1.7712\n",
      "Cost after iteration   30: 0.4075\n",
      "Cost after iteration   40: 0.2108\n",
      "Cost after iteration   50: 0.0707\n",
      "Cost after iteration   60: 0.0386\n",
      "Cost after iteration   70: 0.0221\n",
      "Cost after iteration   80: 0.0143\n",
      "Cost after iteration   90: 0.0099\n",
      "Cost after iteration   99: 0.0073\n"
     ]
    }
   ],
   "source": [
    "print(f'{Tx = }, {X_lexicon_size = }')\n",
    "print(f'{Ty = }, {Y_lexicon_size = }')\n",
    "\n",
    "model, J_hist = fit(\n",
    "    X=X_train_ts, Y=Y_train_ts,\n",
    "    X_lexicon_size=X_lexicon_size,\n",
    "    Y_lexicon_size=Y_lexicon_size, Y_pad_token=Y_lexicon['<pad>'],\n",
    "    alpha=1e-3, num_iters=100, batch_size=256,\n",
    "    teacher_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "\n",
    "ax1.plot(J_hist[:10])\n",
    "ax2.plot(10 + np.arange(len(J_hist[10:])), J_hist[10:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, input_sentence, X_lexicon, Y_inverse_lexicon, Tx=32, Ty=12):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model (torch model)                 : trained seq2seq model\n",
    "        input_sentence (str)                : Input human readable format\n",
    "        X_lexicon (dict(ch:idx))            : Human dictionary\n",
    "        Y_inverse_lexicon (dict(idx:ch))    : Machine inverse dictionary\n",
    "    Returns:\n",
    "        output_sentence (str)               : Predicted machine readable format from model\n",
    "    \"\"\"\n",
    "    model.cpu()\n",
    "    with torch.no_grad():\n",
    "        # str -> [37,2,1,56,38] -> tensor(1, Tx)\n",
    "        X_tensor = torch.LongTensor(\n",
    "            get_feat_tensor([input_sentence], X_lexicon, pad_length=Tx))\n",
    "\n",
    "        # infer: Y_hat = (1, Ty, Y_lexicon_size)\n",
    "        m = X_tensor.size(0)\n",
    "        Y_dummy = torch.zeros(m, Ty).to(torch.int64)\n",
    "        Y_hat = model(X_tensor, Y_dummy,\n",
    "            device='cpu', teacher_force_ratio=0)\n",
    "\n",
    "        # predict\n",
    "        output_sentence = ''\n",
    "\n",
    "        # (1, Ty, Y_lexicon_size) -> (1, Ty) -> (Ty)\n",
    "        y_seq = torch.argmax(Y_hat, dim=2).squeeze(dim=0)\n",
    "        y_seq = y_seq.numpy().tolist()\n",
    "\n",
    "        # skip <start>\n",
    "        for idx in y_seq[1:]:\n",
    "            if Y_inverse_lexicon[idx] == '<end>': break\n",
    "            output_sentence += Y_inverse_lexicon[idx]\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]                        [Prediction]    [Correct Label]\n",
      "4 july 1993                    1993-07-04      1993-07-04     \n",
      "saturday june 21 1997          1997-06-21      1997-06-21     \n",
      "27 08 15                       2015-08-27      2015-08-27     \n",
      "14 05 87                       1987-05-14      1987-05-14     \n",
      "july 16 2022                   2022-07-16      2022-07-16     \n",
      "18 jun 1982                    1982-06-18      1982-06-18     \n",
      "friday february 15 1974        1974-02-15      1974-02-15     \n",
      "31.08.09                       2009-08-31      2009-08-31     \n",
      "thursday april 28 1994         1994-04-28      1994-04-28     \n",
      "5 november 1983                19831-11-05     1983-11-05     \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"[Input]\":30} {\"[Prediction]\":15} {\"[Correct Label]\":15}')\n",
    "for i in range(10):\n",
    "    pred = infer(model, X_test[i],\n",
    "        X_lexicon=X_lexicon, Y_inverse_lexicon=Y_inverse_lexicon,\n",
    "        Tx=Tx, Ty=Ty)\n",
    "    print(f'{X_test[i]:30} {pred:15} {Y_test[i]:15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 95.100%\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "Y_test_pred = Parallel(n_jobs=16)(delayed(function=infer)(model, utt,\n",
    "    X_lexicon=X_lexicon, Y_inverse_lexicon=Y_inverse_lexicon, Tx=Tx, Ty=Ty)\n",
    "        for utt in X_test)\n",
    "\n",
    "# Acc\n",
    "scores = [ 1 if Y_test_pred[i] == Y_test[i] else 0 \\\n",
    "    for i in range(len(Y_test)) ]\n",
    "print(f'Test accuracy = {100.0*sum(scores)/len(Y_test):.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b166c11a6fb13fc284d60599e45a47824480cbed14934159809ec834d0d5166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
