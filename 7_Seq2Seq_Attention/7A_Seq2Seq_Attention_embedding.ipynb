{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "## 1.1 Problem\n",
    "- Translate **human language date** to a `machine standard date` format\n",
    "- Eg:\n",
    "    + **the 29th of August 1958** -> `1958-08-29`\n",
    "    + **03/30/1968**              -> `1968-03-30`\n",
    "    + **24 JUNE 1987**            -> `1987-06-24`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babel.dates import format_date\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# data generator\n",
    "fake = Faker()\n",
    "\n",
    "date_formats = ['short', 'medium', 'long',\n",
    "    'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full',\n",
    "    'd MMM YYY',  'd MMMM YYY', 'dd MMM YYY', 'd MMM, YYY', 'd MMMM, YYY', 'dd, MMM YYY',\n",
    "    'd MM YY', 'd MMMM YYY', 'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY']\n",
    "\n",
    "def generate_training_example():\n",
    "    # Get a random date (standard format)\n",
    "    machine_date = fake.date_object()\n",
    "    \n",
    "    # Generate a human readable format\n",
    "    human_readable = format_date(machine_date, format=random.choice(date_formats), locale='en_US') \\\n",
    "        .lower().replace(',','')\n",
    "    \n",
    "    return human_readable, machine_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 dec 2020                    -> 2020-12-29\n",
      "sunday february 10 1985        -> 1985-02-10\n",
      "4/24/92                        -> 1992-04-24\n",
      "saturday march 17 2018         -> 2018-03-17\n",
      "monday march 7 2022            -> 2022-03-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    human, machine = generate_training_example()\n",
    "    print(f'{human:30} -> {machine}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dset(m=10000):\n",
    "    X, Y = [], []\n",
    "    for i in range(m):\n",
    "        x, y = generate_training_example()\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunday july 30 1995', '06.10.02', 'november 18 2022', 'saturday june 12 1976', '23 august 2008']\n",
      "['1995-07-30', '2002-10-06', '2022-11-18', '1976-06-12', '2008-08-23']\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_dset(m=10000)\n",
    "\n",
    "X_train, Y_train = X[:8000], Y[:8000]\n",
    "X_test, Y_test = X[8000:], Y[8000:]\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lexicon size: 39\n",
      "{' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '<unk>': 35, '<pad>': 36, '<start>': 37, '<end>': 38}\n"
     ]
    }
   ],
   "source": [
    "X_chars = sorted(list(set(''.join(X_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "X_lexicon = { ch:idx for idx, ch in enumerate(X_chars) }\n",
    "X_lexicon_size = len(X_lexicon)\n",
    "\n",
    "print('X lexicon size:', X_lexicon_size)\n",
    "print(X_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y lexicon size: 15\n",
      "Y_lexicon = {'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '<unk>': 11, '<pad>': 12, '<start>': 13, '<end>': 14}\n",
      "Y_inverse_lexicon = {0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<unk>', 12: '<pad>', 13: '<start>', 14: '<end>'}\n"
     ]
    }
   ],
   "source": [
    "Y_chars = sorted(list(set(''.join(Y_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "\n",
    "Y_lexicon         = { ch:idx for idx, ch in enumerate(Y_chars) }\n",
    "Y_lexicon_size    = len(Y_lexicon)\n",
    "Y_inverse_lexicon = { idx:ch for idx, ch in enumerate(Y_chars) }\n",
    "\n",
    "\n",
    "print('Y lexicon size:', Y_lexicon_size)\n",
    "print(f'{Y_lexicon = }')\n",
    "print(f'{Y_inverse_lexicon = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def get_feat_tensor(data, lexicon, pad_length=30):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data    (list(str))         : input data list of utterances\n",
    "        lexicon (dict(char:index))  : lexicon data, categorical encoding char to int\n",
    "        pad_length (int)            : padded length of output utterance \n",
    "\n",
    "    Returns:\n",
    "        data_tensor (ndarray (m, pad_length)) : output tensor with\n",
    "            <m> training seamples\n",
    "            <pad_length> padded utterance size\n",
    "    \"\"\"\n",
    "\n",
    "    # Pipeline for each utterance\n",
    "    def get_utt_tensor(utt:str):\n",
    "        # Tokenize: char to int\n",
    "        utt_tensor = [ lexicon['<start>'] ] + \\\n",
    "            [ lexicon[ch] if lexicon.get(ch) is not None\n",
    "                else lexicon['<unk>']\n",
    "                    for ch in utt ] + \\\n",
    "            [ lexicon['<end>'] ]\n",
    "\n",
    "        # padding\n",
    "        utt_tensor = utt_tensor[:pad_length]\n",
    "        if len(utt_tensor) < pad_length:\n",
    "            utt_tensor += [lexicon['<pad>']]*(pad_length - len(utt_tensor))\n",
    "\n",
    "        return np.array(utt_tensor)\n",
    "\n",
    "    # Convert m examples\n",
    "    tensor = Parallel(n_jobs=16)(delayed(function=get_utt_tensor)(utt)\n",
    "        for utt in data)\n",
    "    return np.array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_ts.shape = (8000, 32)\n",
      "X[0] = [37 29 31 25 16 13 34  0 22 31 23 34  0  6  3  0  4 12 12  8 38 36 36 36\n",
      " 36 36 36 36 36 36 36 36]\n",
      "\n",
      "Y_train_ts.shape = (8000, 17)\n",
      "Y[0] = [13  2 10 10  6  0  1  8  0  4  1 14 12 12 12 12 12]\n"
     ]
    }
   ],
   "source": [
    "# Fixed sequence length\n",
    "Tx = 32\n",
    "Ty = 17\n",
    "\n",
    "X_train_ts = get_feat_tensor(X_train, X_lexicon, pad_length=Tx)\n",
    "Y_train_ts = get_feat_tensor(Y_train, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "X_test_ts = get_feat_tensor(X_test, X_lexicon, pad_length=Tx)\n",
    "Y_test_ts = get_feat_tensor(Y_test, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "# X_train = (m, Tx)\n",
    "print(f'{X_train_ts.shape = }')\n",
    "print('X[0] =', X_train_ts[0], end='\\n\\n')\n",
    "\n",
    "# Y_train = (m, Ty)\n",
    "print(f'{Y_train_ts.shape = }')\n",
    "print('Y[0] =', Y_train_ts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, X_lexicon_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.emb_dim = 64\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        # Emb\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=X_lexicon_size,\n",
    "            embedding_dim=self.emb_dim)\n",
    "\n",
    "        # enc\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "    def forward(self, X, device='cpu'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (Longtensor(m, Tx))                : Input Sequence, type=Long\n",
    "                m: batch_size,\n",
    "                Tx: sequence length \n",
    "        Returns:\n",
    "            o_enc (tensor(m, Tx, 2*hid_dim))         : Encoder output states (bi-directional)\n",
    "            h_enc (tensor(2*num_layers, m, hid_dim)) : Encoder hidden states (bi-directional)\n",
    "            c_enc (tensor(2*num_layers, m, hid_dim)) : Encoder cell states (bi-directional)\n",
    "        \"\"\"\n",
    "        # get batchsize\n",
    "        m = X.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        #    (m, Tx) -> (m, Tx, emb_dim)\n",
    "        emb = self.embedding(X)\n",
    "        emb = F.relu(emb)\n",
    "    \n",
    "        # Init h0, c0\n",
    "        #   (2, m, hid_dim)\n",
    "        h0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "        c0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "\n",
    "        # Encode\n",
    "        #   o_enc: (m, Tx, 2*hid_dim)\n",
    "        #   h_enc/c_enc: (2, m, hid_dim)\n",
    "        o_enc, (h_enc, c_enc) = self.encoder(emb, (h0, c0))\n",
    "        return o_enc, (h_enc, c_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_enc.size() = torch.Size([16, 32, 256])\n",
      "h_enc.size() = torch.Size([2, 16, 128]), c_enc.size() = torch.Size([2, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "X = torch.LongTensor(X_train_ts[:16])\n",
    "\n",
    "encoder = Encoder(X_lexicon_size=39)\n",
    "o_enc, (h_enc, c_enc) = encoder(X)\n",
    "\n",
    "print(f'{o_enc.size() = }')\n",
    "print(f'{h_enc.size() = }, {c_enc.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention - [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<img src=\"./assets/seq2seq_attention.jpg\" width=\"300\"/>\n",
    "\n",
    "- Energy $e_{ij}$\n",
    "    + $[]$: concat ops\n",
    "        + $h_j$: previous decoder hidden state\n",
    "        + $s_{i-1}$: encoder output states\n",
    "    + $f()$: attention function\n",
    "  \n",
    "$$e_{ij} = f( [h_j, s_{i-1}] )$$\n",
    "\n",
    "- Attention (weights)\n",
    "\n",
    "$$\\alpha_{ij} = \\text{softmax}(e_{ij}) = \\frac{exp(e_{ij})}{\\sum\\limits_{k=1}^{Tx}exp(e_{ik})}$$\n",
    "\n",
    "- context vector: **Tell decoder which parts of hidden state which it should pay more attention (weights) to**\n",
    "    $$c_i = \\sum\\limits_{j=1}^{Tx} \\alpha_{ij}h_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.hid_feat_dim = 128\n",
    "\n",
    "        # Aggregate hidden state\n",
    "        self.agg_hidden_fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=self.hid_feat_dim)\n",
    "\n",
    "        # Attention function\n",
    "        self.att_fc = nn.Linear(\n",
    "            in_features=3*self.hid_feat_dim,\n",
    "            out_features=1)\n",
    "\n",
    "    def forward(self, enc_states, ht_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            enc_states (tensor(m, Tx, 2*hid_dim))     : Encoder output states (bi-directional)\n",
    "            ht_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden states (bi-directional)\n",
    "        Returns:\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))  : Context vector\n",
    "        \"\"\"\n",
    "        # get dims: batch_size, sequence_size\n",
    "        m, Tx = enc_states.size(0), enc_states.size(1)\n",
    "\n",
    "        #### Aggregate\n",
    "        # Aggregate ht_dec (bi-directional to 1 dir)\n",
    "        #    (2, m, hid_dim) -> (m, 2*hid_dim) -> (m, hid_dim) \n",
    "        ht_dec_agg = self.agg_hidden_fc(\n",
    "            torch.cat([ht_dec[0], ht_dec[1]], dim=1))\n",
    "\n",
    "        #### Attention\n",
    "        # Reshape ht_dec\n",
    "        #    (m, hid_dim) -> (m, 1, hid_dim) -> (m, Tx, hid_dim) \n",
    "        ht_reshaped = torch.unsqueeze(ht_dec_agg, dim=1) \\\n",
    "            .repeat(1,Tx,1)\n",
    "\n",
    "        # Compute energy e_ij\n",
    "        #   (m, Tx, hid_dim) + (m, Tx, 2*hid_dim) -> (m, Tx, 3*hid_dim)\n",
    "        concat = torch.cat([ht_reshaped, enc_states], dim=2)\n",
    "        #   (m, Tx, 3*hid_dim) -> (m, Tx, 1)\n",
    "        energy = F.relu(self.att_fc(concat))\n",
    "\n",
    "        # Compute attention alpha_ij: (m, Tx, 1)\n",
    "        alpha = F.softmax(energy, dim=2)\n",
    "\n",
    "        # Compute context vector\n",
    "        #   (m, Tx, 1) x (m, Tx, hid_dim) -> (m, 1, 2*hid_dim)\n",
    "        context_vector = torch.einsum(\"mTk,mTn->mkn\", alpha, enc_states)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.size() = torch.Size([16, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention()\n",
    "\n",
    "context = attention(\n",
    "    enc_states=o_enc,\n",
    "    ht_dec=h_enc)\n",
    "\n",
    "print(f'{context.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, Y_lexicon_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.emb_dim = 64\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # Emb\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=Y_lexicon_size,\n",
    "            embedding_dim=self.emb_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=2*self.hid_feat_dim + self.emb_dim,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, yt_prev, context_vector, ht_dec, ct_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            yt_prev (Long tensor(m))  : Output sequence at prev time step (t-1), categorical Long\n",
    "                m: batchsize\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))          : Context vector from attention mechanism\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden/cell states (t-1) (bi-directional)\n",
    "        Returns:\n",
    "            yt_hat (tensor(m, Y_lexicon_size))                : y_hat at timestep t, out_dim = Y_lexicon_size\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : decoder hidden/cell states at current timestep t (bi-directional)\n",
    "        \"\"\"\n",
    "        # (m) -> (m, 1)\n",
    "        yt_prev = yt_prev.unsqueeze(dim=1)\n",
    "\n",
    "        # Embedding\n",
    "        #    (m, 1) -> (m, 1, emb_dim)\n",
    "        emb = self.embedding(yt_prev)\n",
    "        emb = F.relu(emb)\n",
    "\n",
    "        # Decode\n",
    "        #   (m,1,2*hid_dim) + (m,1,emb_dim) -> (m,1,2*hid_dim + emb_dim)\n",
    "        concat = torch.cat([context_vector, emb], dim=2)\n",
    "        #   o_dec:         (m, 1, 2*hid_dim)\n",
    "        #   ht_dec/ct_dec: (2, m, hid_dim)\n",
    "        o_dec, (ht_dec, ct_dec) = self.decoder(concat, (ht_dec, ct_dec))\n",
    "\n",
    "        # Predict y_hat\n",
    "        #   (m, 1, 2*hid_dim) -> (m, 1, y_lexicon_dim) -> (m, y_lexicon_dim)\n",
    "        yt_hat = self.fc(o_dec).squeeze(dim=1)\n",
    "        yt_hat = F.log_softmax(yt_hat, dim=1)\n",
    "\n",
    "        return yt_hat, ht_dec, ct_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.size() = torch.Size([16, 17]), y0.size() = torch.Size([16])\n",
      "y1_hat.size() = torch.Size([16, 15])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.LongTensor(Y_train_ts[:16])\n",
    "y0 = Y[:, 0]\n",
    "print(f'{Y.size() = }, {y0.size() = }')\n",
    "\n",
    "decoder = Decoder(Y_lexicon_size=15)\n",
    "y1_hat, ht_dec, ct_dec = decoder(yt_prev=y0,\n",
    "    context_vector=context,\n",
    "    ht_dec=h_enc, ct_dec=c_enc)\n",
    "\n",
    "print(f'{y1_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Models\n",
    "- Apply context vector to all decoder hidden states\n",
    "\n",
    "<img src=\"./assets/seq2seq_context.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, X_lexicon_size, Y_lexicon_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # Params\n",
    "        self.Y_lexicon_size = Y_lexicon_size\n",
    "\n",
    "        self.encoder = Encoder(X_lexicon_size=X_lexicon_size)\n",
    "        self.attention = Attention()\n",
    "        self.decoder = Decoder(Y_lexicon_size=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, X, Y,\n",
    "            device='cpu', teacher_force_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (Long tensor(m, Tx))        : Input sequence\n",
    "            Y (Long tensor(m, Ty))        : Output sequence\n",
    "            teacher_force_ratio (float)   : teaching forcing probability in range [0,1]\n",
    "        Returns:\n",
    "            Y_hat (tensor(m, Ty, Y_lexicon_size)): Y_hat, out_dim = Y_lexicon_size\n",
    "        \"\"\"\n",
    "        # Get dim\n",
    "        m, Ty = Y.size(0), Y.size(1)\n",
    "\n",
    "        # Encode\n",
    "        enc_out, (h_enc, c_enc) = self.encoder(X, device=device)\n",
    "\n",
    "        # Init yt_prev = <start>, long tensor (m,)\n",
    "        yt_prev = Y[:, 0]\n",
    "\n",
    "        # Init h_dec, c_dec\n",
    "        ht_dec, ct_dec = h_enc, c_enc\n",
    "\n",
    "        # Predict next timestep\n",
    "        Y_hat = torch.zeros(m, Ty, self.Y_lexicon_size).to(device)\n",
    "        for t in range(1, Ty):\n",
    "            # Attention\n",
    "            context = self.attention(\n",
    "                enc_states=enc_out,\n",
    "                ht_dec=ht_dec)\n",
    "\n",
    "            # Decode\n",
    "            Y_hat[:,t,:] , ht_dec, ct_dec = self.decoder(\n",
    "                yt_prev=yt_prev,\n",
    "                context_vector=context,\n",
    "                ht_dec=ht_dec, ct_dec=ct_dec)\n",
    "\n",
    "            # Teaching forcing:\n",
    "            #   prob:      Force yt_prev = Y[t+1]\n",
    "            #   1 - prob:  get prediction from model\n",
    "            if random.random() < teacher_force_ratio:\n",
    "                yt_prev = Y[:,t]\n",
    "            else:\n",
    "                yt_prev = Y_hat[:,t,:].argmax(dim=1)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat.size() = torch.Size([16, 17, 15])\n"
     ]
    }
   ],
   "source": [
    "net = Seq2Seq(X_lexicon_size=39, Y_lexicon_size=15)\n",
    "Y_hat = net(X, Y)\n",
    "\n",
    "print(f'{Y_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit(\n",
    "        X, Y,\n",
    "        X_lexicon_size, Y_lexicon_size, Y_pad_token,\n",
    "        alpha=1e-2, num_iters=1000, batch_size=16, teacher_force_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X (ndarray(m, Tx))      : Input sequence\n",
    "        Y (ndarray(m, Ty))      : Output sequence\n",
    "        X_lexicon_size (int)    : Input sequence feat_dim = size of vocab X.\n",
    "        Y_lexicon_size (int)    : Output sequence feat_dim = size of vocab Y.\n",
    "        Y_pad_token (int)       : Padding token of target sequence, ignore when computing cost\n",
    "    Returns:\n",
    "        net (torch model)       : trained seq2seq model\n",
    "        J_history (list)        : List of cost each iter for plotting\n",
    "    \"\"\"\n",
    "    # Dataset\n",
    "    dset = TensorDataset(\n",
    "        torch.LongTensor(X),\n",
    "        torch.LongTensor(Y))\n",
    "\n",
    "    # Dataloader\n",
    "    dloader = DataLoader(\n",
    "        dataset=dset,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    ## Config\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    net = Seq2Seq(\n",
    "        X_lexicon_size=X_lexicon_size,\n",
    "        Y_lexicon_size=Y_lexicon_size)\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    # Criterions\n",
    "    #    Note: Exclude padding token when compte loss\n",
    "    criterion = nn.NLLLoss(\n",
    "        ignore_index=Y_pad_token)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=alpha)\n",
    "\n",
    "    # cost and params history\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost = 0\n",
    "        for b, batch in enumerate(dloader):\n",
    "            # Batch:\n",
    "            #    X_b = (batch_size, Tx)\n",
    "            #    Y_b = (batch_size, Ty)\n",
    "            Xb, Yb = batch\n",
    "            Xb = Xb.to(device).to(torch.int64)\n",
    "            Yb = Yb.to(device).to(torch.int64)\n",
    "\n",
    "            # Forward\n",
    "            #    Yb_hat = (batch_size, Ty, Y_lexicon_size)\n",
    "            optimizer.zero_grad()\n",
    "            Yb_hat = net(Xb, Yb,\n",
    "                device=device, teacher_force_ratio=teacher_force_ratio)\n",
    "\n",
    "            # Batch Cost compute, t=1 skip <start>\n",
    "            Yb_hat_reshaped = Yb_hat[:,1:,:].reshape(-1, Y_lexicon_size)\n",
    "            Yb_reshaped = Yb[:,1:].reshape(-1)\n",
    "\n",
    "            cost_b = criterion(Yb_hat_reshaped, Yb_reshaped)\n",
    "\n",
    "            # Track Iter Cost\n",
    "            cost += cost_b.item()\n",
    "\n",
    "            # Back Propagation\n",
    "            cost_b.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clip grad to avoid exploding gradient\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "\n",
    "        # Compute Cost\n",
    "        J_history.append(cost)\n",
    "        if i % 10 == 0 or i == num_iters-1:\n",
    "            print(f\"Cost after iteration {i:4}: {cost:.4f}\")\n",
    "    return net, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx = 32, X_lexicon_size = 39\n",
      "Ty = 17, Y_lexicon_size = 15\n",
      "Cost after iteration    0: 67.1745\n",
      "Cost after iteration   10: 8.8624\n",
      "Cost after iteration   20: 1.6997\n",
      "Cost after iteration   30: 0.5100\n",
      "Cost after iteration   40: 0.1897\n",
      "Cost after iteration   50: 0.0846\n",
      "Cost after iteration   60: 0.0452\n",
      "Cost after iteration   70: 0.0337\n",
      "Cost after iteration   80: 0.0171\n",
      "Cost after iteration   90: 0.0113\n",
      "Cost after iteration  100: 0.0082\n",
      "Cost after iteration  110: 0.0061\n",
      "Cost after iteration  119: 0.0048\n"
     ]
    }
   ],
   "source": [
    "print(f'{Tx = }, {X_lexicon_size = }')\n",
    "print(f'{Ty = }, {Y_lexicon_size = }')\n",
    "\n",
    "model, J_hist = fit(\n",
    "    X=X_train_ts, Y=Y_train_ts,\n",
    "    X_lexicon_size=X_lexicon_size,\n",
    "    Y_lexicon_size=Y_lexicon_size, Y_pad_token=Y_lexicon['<pad>'],\n",
    "    alpha=1e-3, num_iters=120, batch_size=256,\n",
    "    teacher_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "\n",
    "ax1.plot(J_hist[:10])\n",
    "ax2.plot(10 + np.arange(len(J_hist[10:])), J_hist[10:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, input_sentence,\n",
    "        X_lexicon,\n",
    "        Y_lexicon, Y_inverse_lexicon,\n",
    "        Tx=32, Ty=12):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model (torch model)                 : trained seq2seq model\n",
    "        input_sentence (str)                : Input human readable format\n",
    "        X_lexicon (dict(ch:idx))            : Human dictionary\n",
    "        Y_lexicon (dict(ch:idx))            : Machine dictionary\n",
    "        Y_inverse_lexicon (dict(idx:ch))    : Machine inverse dictionary\n",
    "    Returns:\n",
    "        output_sentence (str)               : Predicted machine readable format from model\n",
    "    \"\"\"\n",
    "    # str -> [37,2,1,56,38] -> tensor(1, Tx)\n",
    "    X_seq = torch.LongTensor(\n",
    "        get_feat_tensor([input_sentence], X_lexicon, pad_length=Tx))\n",
    "\n",
    "    # Y_in = (1, Ty)\n",
    "    m = X_seq.size(0)\n",
    "    Y_in = torch.full((m, Ty), fill_value=Y_lexicon['<start>'], dtype=torch.int64)\n",
    "    \n",
    "    # infer: Y_hat = (1, Ty, Y_lexicon_size)\n",
    "    with torch.no_grad():        \n",
    "        Y_hat = model(X_seq, Y_in,\n",
    "            device='cpu', teacher_force_ratio=0)\n",
    "\n",
    "    # (1, Ty, Y_lexicon_size) -> (1, Ty) -> (Ty)\n",
    "    y_seq = torch.argmax(Y_hat, dim=2).squeeze(dim=0)\n",
    "    y_seq = y_seq.numpy().tolist()\n",
    "\n",
    "    # Retrieve best guess\n",
    "    output_sentence = ''\n",
    "    # skip <start>\n",
    "    for idx in y_seq[1:]:\n",
    "        if Y_inverse_lexicon[idx] == '<end>': break\n",
    "        output_sentence += Y_inverse_lexicon[idx]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]                        [Prediction]    [Correct Label]\n",
      "06 nov 1989                    1989-11-06      1989-11-06     \n",
      "tuesday march 7 2017           2017-03-07      2017-03-07     \n",
      "21 03 16                       2016-03-21      2016-03-21     \n",
      "wednesday october 17 1973      1973-10-17      1973-10-17     \n",
      "february 6 1993                1993-02-06      1993-02-06     \n",
      "friday october 5 1984          1984-10-05      1984-10-05     \n",
      "may 15 2019                    2019-05-15      2019-05-15     \n",
      "7 june 2018                    2018-06-07      2018-06-07     \n",
      "05 may 2005                    2005-05-05      2005-05-05     \n",
      "mar 21 1988                    1988-03-21      1988-03-21     \n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print(f'{\"[Input]\":30} {\"[Prediction]\":15} {\"[Correct Label]\":15}')\n",
    "for i in range(10):\n",
    "    pred = infer(model, X_test[i],\n",
    "        X_lexicon=X_lexicon,\n",
    "        Y_lexicon=Y_lexicon, Y_inverse_lexicon=Y_inverse_lexicon,\n",
    "        Tx=Tx, Ty=Ty)\n",
    "    print(f'{X_test[i]:30} {pred:15} {Y_test[i]:15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 99.150%\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "Y_test_pred = Parallel(n_jobs=2)(delayed(function=infer)(model, utt,\n",
    "    X_lexicon=X_lexicon,\n",
    "    Y_lexicon=Y_lexicon, Y_inverse_lexicon=Y_inverse_lexicon,\n",
    "    Tx=Tx, Ty=Ty)\n",
    "        for utt in X_test)\n",
    "\n",
    "# Acc\n",
    "scores = [ 1 if Y_test_pred[i] == Y_test[i] else 0 \\\n",
    "    for i in range(len(Y_test)) ]\n",
    "print(f'Test accuracy = {100.0*sum(scores)/len(Y_test):.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b166c11a6fb13fc284d60599e45a47824480cbed14934159809ec834d0d5166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
