{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "## 1.1 Problem\n",
    "- Translate **human language date** to a `machine standard date` format\n",
    "- Eg:\n",
    "    + **the 29th of August 1958** -> `1958-08-29`\n",
    "    + **03/30/1968**              -> `1968-03-30`\n",
    "    + **24 JUNE 1987**            -> `1987-06-24`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babel.dates import format_date\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# data generator\n",
    "fake = Faker()\n",
    "\n",
    "date_formats = ['short', 'medium', 'long',\n",
    "    'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full',\n",
    "    'd MMM YYY',  'd MMMM YYY', 'dd MMM YYY', 'd MMM, YYY', 'd MMMM, YYY', 'dd, MMM YYY',\n",
    "    'd MM YY', 'd MMMM YYY', 'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY']\n",
    "\n",
    "def generate_training_example():\n",
    "    # Get a random date (standard format)\n",
    "    machine_date = fake.date_object()\n",
    "\n",
    "    # Generate a human readable format\n",
    "    human_readable = format_date(machine_date, format=random.choice(date_formats), locale='en_US') \\\n",
    "        .lower().replace(',','')\n",
    "\n",
    "    return human_readable, machine_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monday july 27 1998            -> 1998-07-27\n",
      "07 apr 1979                    -> 1979-04-07\n",
      "14 oct 1974                    -> 1974-10-14\n",
      "tuesday october 4 2005         -> 2005-10-04\n",
      "july 9 1993                    -> 1993-07-09\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    human, machine = generate_training_example()\n",
    "    print(f'{human:30} -> {machine}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dset(m=10000):\n",
    "    X, Y = [], []\n",
    "    for i in range(m):\n",
    "        x, y = generate_training_example()\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16 nov 1996', 'november 12 1991', '8 september 2002', 'tuesday july 7 1981', '12/14/00']\n",
      "['1996-11-16', '1991-11-12', '2002-09-08', '1981-07-07', '2000-12-14']\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_dset(m=10000)\n",
    "\n",
    "X_train, Y_train = X[:8000], Y[:8000]\n",
    "X_test, Y_test = X[8000:], Y[8000:]\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lexicon size: 39\n",
      "{' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '<unk>': 35, '<pad>': 36, '<start>': 37, '<end>': 38}\n"
     ]
    }
   ],
   "source": [
    "X_chars = sorted(list(set(''.join(X_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "X_lexicon = { ch:idx for idx, ch in enumerate(X_chars) }\n",
    "X_lexicon_size = len(X_lexicon)\n",
    "\n",
    "print('X lexicon size:', X_lexicon_size)\n",
    "print(X_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y lexicon size: 15\n",
      "Y_lexicon = {'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '<unk>': 11, '<pad>': 12, '<start>': 13, '<end>': 14}\n",
      "Y_inverse_lexicon = {0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<unk>', 12: '<pad>', 13: '<start>', 14: '<end>'}\n"
     ]
    }
   ],
   "source": [
    "Y_chars = sorted(list(set(''.join(Y_train)))) + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "\n",
    "Y_lexicon         = { ch:idx for idx, ch in enumerate(Y_chars) }\n",
    "Y_lexicon_size    = len(Y_lexicon)\n",
    "Y_inverse_lexicon = { idx:ch for idx, ch in enumerate(Y_chars) }\n",
    "\n",
    "\n",
    "print('Y lexicon size:', Y_lexicon_size)\n",
    "print(f'{Y_lexicon = }')\n",
    "print(f'{Y_inverse_lexicon = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def get_feat_tensor(data, lexicon, pad_length=30):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data    (list(str))         : input data list of utterances\n",
    "        lexicon (dict(char:index))  : lexicon data, categorical encoding char to int\n",
    "        pad_length (int)            : padded length of output utterance \n",
    "\n",
    "    Returns:\n",
    "        data_tensor (ndarray (m, pad_length)) : output tensor with\n",
    "            <m> training seamples\n",
    "            <pad_length> padded utterance size\n",
    "    \"\"\"\n",
    "    # one-hot encoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    ohe = OneHotEncoder(categories=[[idx for idx in lexicon.values()]])\n",
    "\n",
    "    # Pipeline for each utterance\n",
    "    def get_utt_tensor(utt:str):\n",
    "        # Tokenize: char to int\n",
    "        utt_tensor = [ lexicon['<start>'] ] + \\\n",
    "            [ lexicon[ch] if lexicon.get(ch) is not None\n",
    "                else lexicon['<unk>']\n",
    "                    for ch in utt ] + \\\n",
    "            [ lexicon['<end>'] ]\n",
    "\n",
    "        # padding\n",
    "        utt_tensor = utt_tensor[:pad_length]\n",
    "        if len(utt_tensor) < pad_length:\n",
    "            utt_tensor += [lexicon['<pad>']]*(pad_length - len(utt_tensor))\n",
    "\n",
    "        # Embedding: int to one-hot vector\n",
    "        utt_tensor = list(map(lambda utt: [utt], utt_tensor))\n",
    "        utt_tensor = ohe.fit_transform(utt_tensor).toarray()\n",
    "\n",
    "        return utt_tensor\n",
    "\n",
    "    # Convert m examples\n",
    "    tensor = Parallel(n_jobs=16)(delayed(function=get_utt_tensor)(utt)\n",
    "        for utt in data)\n",
    "    return np.array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_ts.shape = (8000, 32, 39)\n",
      "X[0] = [[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "Y_train_ts.shape = (8000, 17, 15)\n",
      "Y[0] = [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Fixed sequence length\n",
    "Tx = 32\n",
    "Ty = 17\n",
    "\n",
    "X_train_ts = get_feat_tensor(X_train, X_lexicon, pad_length=Tx)\n",
    "Y_train_ts = get_feat_tensor(Y_train, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "X_test_ts = get_feat_tensor(X_test, X_lexicon, pad_length=Tx)\n",
    "Y_test_ts = get_feat_tensor(Y_test, Y_lexicon, pad_length=Ty)\n",
    "\n",
    "# X_train = (m, Tx)\n",
    "print(f'{X_train_ts.shape = }')\n",
    "print('X[0] =', X_train_ts[0], end='\\n\\n')\n",
    "\n",
    "# Y_train = (m, Ty)\n",
    "print(f'{Y_train_ts.shape = }')\n",
    "print('Y[0] =', Y_train_ts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, X_lexicon_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        # params\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "\n",
    "         # enc\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=X_lexicon_size,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "    def forward(self, X, device='cpu'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (tensor(m, Tx, X_lexicon_size))        : Input Sequence, type=Float\n",
    "                m                 : batch_size,\n",
    "                Tx                : sequence length\n",
    "                X_lexicon_size    : Input one-hot feat = X_lexicon_size\n",
    "        Returns:\n",
    "            o_enc (tensor(m, Tx, 2*hid_dim))         : Encoder output states (bi-directional)\n",
    "            h_enc (tensor(2*num_layers, m, hid_dim)) : Encoder hidden states (bi-directional)\n",
    "            c_enc (tensor(2*num_layers, m, hid_dim)) : Encoder cell states (bi-directional)\n",
    "        \"\"\"\n",
    "        # get batchsize\n",
    "        m = X.size(0)\n",
    "\n",
    "        # Init h0, c0\n",
    "        #   (2, m, hid_dim)\n",
    "        h0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "        c0 = torch.zeros(2*self.num_layers, m, self.hid_feat_dim) \\\n",
    "            .float().to(device)\n",
    "\n",
    "        # Encode\n",
    "        #   o_enc: (m, Tx, 2*hid_dim)\n",
    "        #   h_enc/c_enc: (2, m, hid_dim)\n",
    "        o_enc, (h_enc, c_enc) = self.encoder(X, (h0, c0))\n",
    "        return o_enc, (h_enc, c_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_enc.size() = torch.Size([16, 32, 256])\n",
      "h_enc.size() = torch.Size([2, 16, 128]), c_enc.size() = torch.Size([2, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor(X_train_ts[:16,:,:])\n",
    "\n",
    "encoder = Encoder(X_lexicon_size=39)\n",
    "o_enc, (h_enc, c_enc) = encoder(X)\n",
    "\n",
    "print(f'{o_enc.size() = }')\n",
    "print(f'{h_enc.size() = }, {c_enc.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention - [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<img src=\"./assets/seq2seq_attention.jpg\" width=\"300\"/>\n",
    "\n",
    "- Energy $e_{ij}$\n",
    "    + $[]$: concat ops\n",
    "        + $h_j$: previous decoder hidden state\n",
    "        + $s_{i-1}$: encoder output states\n",
    "    + $f()$: attention function\n",
    "\n",
    "$$e_{ij} = f( [h_j, s_{i-1}] )$$\n",
    "\n",
    "- Attention weights\n",
    "\n",
    "$$\\alpha_{ij} = \\text{softmax}(e_{ij}) = \\frac{exp(e_{ij})}{\\sum\\limits_{k=1}^{Tx}exp(e_{ik})}$$\n",
    "\n",
    "- context vector: **Tell decoder which parts of hidden state which it should pay more attention (weights) to**\n",
    "    $$c_i = \\sum\\limits_{j=1}^{Tx} \\alpha_{ij}h_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.hid_feat_dim = 128\n",
    "\n",
    "        # Aggregate hidden state\n",
    "        self.agg_hidden_fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=self.hid_feat_dim)\n",
    "\n",
    "        # Attention function\n",
    "        self.att_fc = nn.Linear(\n",
    "            in_features=3*self.hid_feat_dim,\n",
    "            out_features=1)\n",
    "\n",
    "    def forward(self, enc_states, ht_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            enc_states (tensor(m, Tx, 2*hid_dim))     : Encoder output states (bi-directional)\n",
    "            ht_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden states (bi-directional)\n",
    "        Returns:\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))  : Context vector\n",
    "        \"\"\"\n",
    "        # get dims: batch_size, sequence_size\n",
    "        m, Tx = enc_states.size(0), enc_states.size(1)\n",
    "\n",
    "        #### Aggregate\n",
    "        # Aggregate ht_dec (bi-directional to 1 dir)\n",
    "        #    (2, m, hid_dim) -> (m, 2*hid_dim) -> (m, hid_dim)\n",
    "        ht_dec_agg = self.agg_hidden_fc(\n",
    "            torch.cat([ht_dec[0], ht_dec[1]], dim=1))\n",
    "\n",
    "        #### Attention\n",
    "        # Reshape ht_dec\n",
    "        #    (m, hid_dim) -> (m, 1, hid_dim) -> (m, Tx, hid_dim)\n",
    "        ht_reshaped = torch.unsqueeze(ht_dec_agg, dim=1) \\\n",
    "            .repeat(1,Tx,1)\n",
    "\n",
    "        # Compute energy e_ij\n",
    "        #   (m, Tx, hid_dim) + (m, Tx, 2*hid_dim) -> (m, Tx, 3*hid_dim)\n",
    "        concat = torch.cat([ht_reshaped, enc_states], dim=2)\n",
    "        #   (m, Tx, 3*hid_dim) -> (m, Tx, 1)\n",
    "        energy = F.relu(self.att_fc(concat))\n",
    "\n",
    "        # Compute attention weights alpha_ij: (m, Tx, 1)\n",
    "        alpha = F.softmax(energy, dim=2)\n",
    "\n",
    "        # Compute context vector\n",
    "        #   (m, Tx, 1) x (m, Tx, hid_dim) -> (m, 1, 2*hid_dim)\n",
    "        context_vector = torch.einsum(\"mTk,mTn->mkn\", alpha, enc_states)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.size() = torch.Size([16, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention()\n",
    "\n",
    "context = attention(\n",
    "    enc_states=o_enc,\n",
    "    ht_dec=h_enc)\n",
    "\n",
    "print(f'{context.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, Y_lexicon_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # params\n",
    "        self.hid_feat_dim = 128\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=2*self.hid_feat_dim + Y_lexicon_size,\n",
    "            hidden_size=self.hid_feat_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, dropout=0)\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=2*self.hid_feat_dim,\n",
    "            out_features=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, yt_prev, context_vector, ht_dec, ct_dec):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            yt_prev (tensor(m, Y_lexicon_size))    : Output sequence at prev time step (t-1), onehot vector, onehot_dim = Y_lexicon_size\n",
    "                m : batch_size\n",
    "            context_vector (tensor(m, 1, 2*hid_dim))          : Context vector from attention mechanism\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : Previous decoder hidden/cell states (t-1) (bi-directional)\n",
    "        Returns:\n",
    "            yt_hat (tensor(m, Y_lexicon_size))  : y_hat at timestep t, out_dim = Y_lexicon_size\n",
    "            ht_dec, ct_dec (tensor(2*num_layers, m, hid_dim)) : decoder hidden/cell states at current timestep t (bi-directional)\n",
    "        \"\"\"\n",
    "        # (m, onehot_dim) -> (m, 1, onehot_dim)\n",
    "        yt_prev = yt_prev.unsqueeze(dim=1)\n",
    "\n",
    "        # Decode\n",
    "        #   (m,1,2*hid_dim) + (m, 1, onehot_dim) -> (m,1,2*hid_dim + onehot_dim)\n",
    "        concat = torch.cat([context_vector, yt_prev], dim=2)\n",
    "        #   o_dec:         (m, 1, 2*hid_dim)\n",
    "        #   ht_dec/ct_dec: (2, m, hid_dim)\n",
    "        o_dec, (ht_dec, ct_dec) = self.decoder(concat, (ht_dec, ct_dec))\n",
    "\n",
    "        # Predict y_hat\n",
    "        #   (m, 1, 2*hid_dim) -> (m, 1, y_lexicon_dim) -> (m, y_lexicon_dim)\n",
    "        yt_hat = self.fc(o_dec).squeeze(dim=1)\n",
    "        yt_hat = F.log_softmax(yt_hat, dim=1)\n",
    "\n",
    "        return yt_hat, ht_dec, ct_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.size() = torch.Size([16, 17, 15]), y0.size() = torch.Size([16, 15])\n",
      "y1_hat.size() = torch.Size([16, 15])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.Tensor(Y_train_ts[:16])\n",
    "y0 = Y[:, 0]\n",
    "print(f'{Y.size() = }, {y0.size() = }')\n",
    "\n",
    "decoder = Decoder(Y_lexicon_size=15)\n",
    "y1_hat, ht_dec, ct_dec = decoder(yt_prev=y0,\n",
    "    context_vector=context,\n",
    "    ht_dec=h_enc, ct_dec=c_enc)\n",
    "\n",
    "print(f'{y1_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Models\n",
    "- Apply context vector to all decoder hidden states\n",
    "\n",
    "<img src=\"./assets/seq2seq_context.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, X_lexicon_size, Y_lexicon_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(X_lexicon_size=X_lexicon_size)\n",
    "        self.attention = Attention()\n",
    "        self.decoder = Decoder(Y_lexicon_size=Y_lexicon_size)\n",
    "\n",
    "    def forward(self, X, Y,\n",
    "            device='cpu', teacher_force_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (Long tensor(m, Tx, X_lexicon_size))        : Input sequence\n",
    "            Y (Long tensor(m, Ty, Y_lexicon_size))        : Output sequence\n",
    "            teacher_force_ratio (float)   : teaching forcing probability in range [0,1]\n",
    "        Returns:\n",
    "            Y_hat (tensor(m, Ty, Y_lexicon_size)) : Y_hat, out_dim = Y_lexicon_size\n",
    "        \"\"\"\n",
    "        # Get dim\n",
    "        m, Ty, Y_lexicon_size = Y.size()\n",
    "\n",
    "        # Encode\n",
    "        enc_out, (h_enc, c_enc) = self.encoder(X, device=device)\n",
    "\n",
    "        # Init yt_prev = <start>, (m, Y_lexicon_size)\n",
    "        yt_prev = Y[:,0,:]\n",
    "\n",
    "        # Init h_dec, c_dec\n",
    "        ht_dec, ct_dec = h_enc, c_enc\n",
    "\n",
    "        # Predict next timestep\n",
    "        Y_hat = torch.zeros(m, Ty, Y_lexicon_size).to(device)\n",
    "        for t in range(1, Ty):\n",
    "            # Attention\n",
    "            context = self.attention(\n",
    "                enc_states=enc_out,\n",
    "                ht_dec=ht_dec)\n",
    "\n",
    "            # Decode\n",
    "            Y_hat[:,t,:] , ht_dec, ct_dec = self.decoder(\n",
    "                yt_prev=yt_prev,\n",
    "                context_vector=context,\n",
    "                ht_dec=ht_dec, ct_dec=ct_dec)\n",
    "\n",
    "            # Teaching forcing:\n",
    "            #   prob:      Force yt_prev = Y[t+1]\n",
    "            #   1 - prob:  get prediction from model\n",
    "            if random.random() < teacher_force_ratio:\n",
    "                yt_prev = Y[:,t,:]\n",
    "            else:\n",
    "                yt_prev = Y_hat[:,t,:]\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat.size() = torch.Size([16, 17, 15])\n"
     ]
    }
   ],
   "source": [
    "net = Seq2Seq(X_lexicon_size=39, Y_lexicon_size=15)\n",
    "Y_hat = net(X, Y)\n",
    "\n",
    "print(f'{Y_hat.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit(\n",
    "        X, Y,\n",
    "        alpha=1e-2, num_iters=1000, batch_size=16, teacher_force_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X (ndarray(m, Tx, X_lexicon_size))  : Input sequence\n",
    "        Y (ndarray(m, Ty, Y_lexicon_size))  : Output sequence\n",
    "    Returns:\n",
    "        net (torch model)       : trained seq2seq model\n",
    "        J_history (list)        : List of cost each iter for plotting\n",
    "    \"\"\"\n",
    "    # Params\n",
    "    X_lexicon_size = X.shape[2]\n",
    "    Y_lexicon_size = Y.shape[2]\n",
    "\n",
    "    # Dataset\n",
    "    dset = TensorDataset(\n",
    "        torch.Tensor(X),\n",
    "        torch.Tensor(Y))\n",
    "\n",
    "    # Dataloader\n",
    "    dloader = DataLoader(\n",
    "        dataset=dset,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    ## Config\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    net = Seq2Seq(\n",
    "        X_lexicon_size=X_lexicon_size,\n",
    "        Y_lexicon_size=Y_lexicon_size)\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=alpha)\n",
    "\n",
    "    # cost and params history\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost = 0\n",
    "        for b, batch in enumerate(dloader):\n",
    "            # Batch:\n",
    "            #    X_b = (batch_size, Tx, X_lexicon_size)\n",
    "            #    Y_b = (batch_size, Ty, Y_lexicon_size)\n",
    "            Xb, Yb = batch\n",
    "            Xb = Xb.to(device).to(torch.float32)\n",
    "            Yb = Yb.to(device).to(torch.float32)\n",
    "\n",
    "            # Forward\n",
    "            #    Yb_hat = (batch_size, Ty, Y_lexicon_size)\n",
    "            optimizer.zero_grad()\n",
    "            Yb_hat = net(Xb, Yb,\n",
    "                device=device, teacher_force_ratio=teacher_force_ratio)\n",
    "\n",
    "            # Batch Cost compute, t=1 skip <start>\n",
    "            Yb_hat_reshaped = Yb_hat[:,1:,:].reshape(-1, Y_lexicon_size)\n",
    "            Yb_reshaped = Yb[:,1:].argmax(dim=2).reshape(-1)\n",
    "\n",
    "            cost_b = criterion(Yb_hat_reshaped, Yb_reshaped)\n",
    "\n",
    "            # Track Iter Cost\n",
    "            cost += cost_b.item()\n",
    "\n",
    "            # Back Propagation\n",
    "            cost_b.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clip grad to avoid exploding gradient\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "\n",
    "        # Compute Cost\n",
    "        J_history.append(cost)\n",
    "        if i % 10 == 0 or i == num_iters-1:\n",
    "            print(f\"Cost after iteration {i:4}: {cost:.4f}\")\n",
    "    return net, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx = 32, X_lexicon_size = 39\n",
      "Ty = 17, Y_lexicon_size = 15\n",
      "Cost after iteration    0: 65.3915\n",
      "Cost after iteration   10: 26.1351\n",
      "Cost after iteration   20: 6.3290\n",
      "Cost after iteration   30: 1.9546\n",
      "Cost after iteration   40: 0.7018\n",
      "Cost after iteration   50: 0.2915\n",
      "Cost after iteration   60: 0.1938\n",
      "Cost after iteration   70: 0.0939\n",
      "Cost after iteration   80: 0.0574\n",
      "Cost after iteration   90: 0.0365\n",
      "Cost after iteration  100: 0.0275\n",
      "Cost after iteration  110: 0.0211\n",
      "Cost after iteration  119: 0.0153\n"
     ]
    }
   ],
   "source": [
    "print(f'{Tx = }, {X_lexicon_size = }')\n",
    "print(f'{Ty = }, {Y_lexicon_size = }')\n",
    "\n",
    "model, J_hist = fit(\n",
    "    X=X_train_ts, Y=Y_train_ts,\n",
    "    alpha=1e-3, num_iters=120, batch_size=256,\n",
    "    teacher_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "\n",
    "ax1.plot(J_hist[:10])\n",
    "ax2.plot(10 + np.arange(len(J_hist[10:])), J_hist[10:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, input_sentence,\n",
    "          X_lexicon,\n",
    "          Y_lexicon, Y_inverse_lexicon,\n",
    "          Tx=32, Ty=12):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model (torch model)                 : trained seq2seq model\n",
    "        input_sentence (str)                : Input human readable format\n",
    "        X_lexicon (dict(ch:idx))            : Human dictionary\n",
    "        Y_lexicon (dict(ch:idx))            : Machine dictionary\n",
    "        Y_inverse_lexicon (dict(idx:ch))    : Machine inverse dictionary\n",
    "    Returns:\n",
    "        output_sentence (str)               : Predicted machine readable format from model\n",
    "    \"\"\"\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    Y_lexicon_size = len(Y_inverse_lexicon)\n",
    "\n",
    "    # str -> [37,2,1,56,38] -> tensor(1, Tx, X_lexicon_size)\n",
    "    X_tensor = torch.Tensor(\n",
    "        get_feat_tensor([input_sentence], X_lexicon, pad_length=Tx))\n",
    "    m = X_tensor.size(0) # m = 1\n",
    "\n",
    "    # Y_in = (1, Ty, Y_lexicon_size)\n",
    "    m = X_tensor.size(0)\n",
    "    Y_in = torch.zeros(m, Ty, Y_lexicon_size).to(torch.int64)\n",
    "    Y_in[:,0,Y_lexicon['<start>']] = 1\n",
    "\n",
    "    # infer: Y_hat = (1, Ty, Y_lexicon_size)\n",
    "    with torch.no_grad():\n",
    "        Y_hat = model(X_tensor, Y_in,\n",
    "            device='cpu', teacher_force_ratio=0)\n",
    "\n",
    "    # (1, Ty, Y_lexicon_size) -> (1, Ty) -> (Ty)\n",
    "    y_seq = torch.argmax(Y_hat, dim=2).squeeze(dim=0)\n",
    "    y_seq = y_seq.numpy().tolist()\n",
    "\n",
    "    # predict\n",
    "    output_sentence = ''\n",
    "    # skip <start>\n",
    "    for idx in y_seq[1:]:\n",
    "        if Y_inverse_lexicon[idx] == '<end>': break\n",
    "        output_sentence += Y_inverse_lexicon[idx]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]                        [Prediction]    [Correct Label]\n",
      "12 november 1978               1978-11-12      1978-11-12     \n",
      "2 may 1991                     1991-05-02      1991-05-02     \n",
      "12/1/86                        1986-12-02      1986-12-01     \n",
      "tuesday april 10 1979          1979-04-10      1979-04-10     \n",
      "15 jun 1997                    1997-06-15      1997-06-15     \n",
      "18 august 2011                 2011-08-18      2011-08-18     \n",
      "5 03 71                        1971-03-05      1971-03-05     \n",
      "15.08.76                       1976-08-15      1976-08-15     \n",
      "10/21/01                       2001-10-20      2001-10-21     \n",
      "10 september 2001              2001-09-10      2001-09-10     \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"[Input]\":30} {\"[Prediction]\":15} {\"[Correct Label]\":15}')\n",
    "for i in range(10):\n",
    "    pred = infer(model, X_test[i],\n",
    "        X_lexicon=X_lexicon,\n",
    "        Y_lexicon=Y_lexicon, Y_inverse_lexicon=Y_inverse_lexicon,\n",
    "        Tx=Tx, Ty=Ty)\n",
    "    print(f'{X_test[i]:30} {pred:15} {Y_test[i]:15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 99.100%\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "Y_test_pred = Parallel(n_jobs=2)(delayed(function=infer)(model, utt,\n",
    "    X_lexicon=X_lexicon,\n",
    "    Y_lexicon=Y_lexicon, Y_inverse_lexicon=Y_inverse_lexicon,\n",
    "    Tx=Tx, Ty=Ty)\n",
    "        for utt in X_test)\n",
    "\n",
    "# Acc\n",
    "scores = [ 1 if Y_test_pred[i] == Y_test[i] else 0 \\\n",
    "    for i in range(len(Y_test)) ]\n",
    "print(f'Test accuracy = {100.0*sum(scores)/len(Y_test):.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b166c11a6fb13fc284d60599e45a47824480cbed14934159809ec834d0d5166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
