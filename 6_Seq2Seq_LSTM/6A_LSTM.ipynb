{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "#### Notation\n",
    "\n",
    "$$x_j^{(i)[l]\\langle t \\rangle}$$\n",
    "- $[l]$: layer l-th (L layers)\n",
    "+ $(i)$: data point i-th (m data points)\n",
    "+ $\\langle t \\rangle$: $\\text{t}^\\text{th}$ timestep (T timesteps)\n",
    "+ $j$: feature j-th (n features)\n",
    "\n",
    "$$[a, x]:\\ \\text{concatenate a with x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forward\n",
    "\n",
    "## 1.1 LSTM Cell\n",
    "<img src=\"./assets/LSTM_cell.png\" width=\"550\"/>\n",
    "\n",
    "#### Forget gate $\\mathbf{\\Gamma}_{f}$\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "\n",
    "- Forget gate = a tensor containing values that are between 0 and 1 (sigmoid)\n",
    "    + $\\approx 0$ = forget/supress corresponding values in $\\mathbf{c}^{\\langle t-1 \\rangle}$\n",
    "    + $\\approx 1$ = Remember corresponding values in $\\mathbf{c}^{\\langle t-1 \\rangle}$\n",
    "- Example: an LSTM to keep track of grammatical structures\n",
    "    + if a subject change from singular (\"puppy\") to plural (\"puppies\")\n",
    "    + the memory of the previous state becomes outdated -> forget that state\n",
    "\n",
    "- Note:\n",
    "    + $\\mathbf{\\Gamma}_f^{\\langle t \\rangle} * \\mathbf{c}^{\\langle t-1 \\rangle}$ = applying a mask over the previous cell state.\n",
    "\n",
    "#### Update gate $\\mathbf{\\Gamma}_{i}$\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "- Update gate decide which parts of the candidate tensor $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to the cell state $c^{\\langle t \\rangle}$\n",
    "- Update gate = a tensor containing values between 0 and 1 (sigmoid)\n",
    "    + $\\approx 0$ = Prevents corresponding values in $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ from being passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "    + $\\approx 1$ = Allow corresponding values in $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to be passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "- Note:\n",
    "    + $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} * \\tilde{c}^{\\langle t \\rangle}$ is used in determining the cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "    \n",
    "#### Candidate value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "- The candidate value = a tensor containing information from the current time step that **may** be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "    + Which parts of the candidate value get passed on depends on the update gate.\n",
    "+ The candidate value = a tensor containing values that range from -1 to 1. (tanh)\n",
    "\n",
    "#### Output gate $\\mathbf{\\Gamma}_{o}$\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{4}$$ \n",
    "\n",
    "- Output gate = a tensor contains values that range from 0 to 1 (sigmoid)\n",
    "\n",
    "- Output gate determined by the previous hidden state $\\mathbf{a}^{\\langle t-1 \\rangle}$ and the current input $\\mathbf{x}^{\\langle t \\rangle}$\n",
    "\n",
    "#### Cell (Memory) state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{5} $$\n",
    "\n",
    "- The cell state is the memory that gets passed onto future time steps.\n",
    "- The previous cell state $\\mathbf{c}^{\\langle t-1 \\rangle}$ is adjusted (weighted) by the forget gate $\\mathbf{\\Gamma}_{f}^{\\langle t \\rangle}$\n",
    "- The candidate value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$, adjusted (weighted) by the update gate $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Hidden state $\\mathbf{a}^{\\langle t \\rangle}$\n",
    "\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "- The hidden state $\\mathbf{a}^{\\langle t \\rangle}$ is determined by the cell state $\\mathbf{c}^{\\langle t \\rangle}$ and the output gate $\\mathbf{\\Gamma}_{o}$\n",
    "- The hidden state determines\n",
    "    + $\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$ in the next time step\n",
    "    + the prediction $y^{\\langle t \\rangle}$ in the current timestep\n",
    "- Note\n",
    "    + The cell state $\\mathbf{c}^{\\langle t \\rangle}$ is passed through the \"tanh\" function to rescale values between -1 and +1.\n",
    "    + The output gate acts like a \"mask\" to the values of $\\tanh(\\mathbf{c}^{\\langle t \\rangle})$\n",
    "    \n",
    "#### Prediction $\\mathbf{\\hat{y}}^{\\langle t \\rangle}$\n",
    "\n",
    "$$\\mathbf{\\hat{y}}^{\\langle t \\rangle} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "- Depend on the application to choose the activation. In this case classification -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def softmax(Z):\n",
    "    ez = np.exp(Z)\n",
    "    return ez / np.sum(ez, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        xt (ndarray (n_x, m))       : your input data at timestep \"t\"\n",
    "        a_prev (ndarray (n_a, m))   : Hidden state at timestep \"t-1\"\n",
    "        c_prev (ndarray (n_a, m))   : Memory state at timestep \"t-1\"\n",
    "        parameters (dict) :\n",
    "            Wf (ndarray (n_a, n_a + n_x))    : Weight matrix of the forget gate\n",
    "            bf (ndarray (n_a, 1))            : Bias of the forget gate\n",
    "            Wi (ndarray (n_a, n_a + n_x))    : Weight matrix of the update gate\n",
    "            bi (ndarray (n_a, 1))            : Bias of the update gate\n",
    "            Wc (ndarray (n_a, n_a + n_x))    : Weight matrix of c~\n",
    "            bc (ndarray (n_a, 1))            : Bias of c~\n",
    "            Wo (ndarray (n_a, n_a + n_x))    : Weight matrix of the output gate\n",
    "            bo (ndarray (n_a, 1))            : Bias of the output gate\n",
    "            Wy (ndarray (n_y, n_a))          : Weight matrix of prediction y_hat\n",
    "            by (ndarray (n_y, 1))            : Bias of prediction y_hat\n",
    "                        \n",
    "    Returns:\n",
    "        a_next (ndarray (n_a, m))   : next hidden state timestep \"t\"\n",
    "        c_next (ndarray (n_a, m))   : next memory state timestep \"t\" \n",
    "        yt_hat (ndarray (n_y, m))   : prediction at timestep \"t\"\n",
    "        cache (tuple)               : contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "                                      needed for the backward pass\n",
    "\n",
    "    Note:\n",
    "        ft/it/ot stand for the forget/update/output gates\n",
    "        cct stands for the candidate value (c~),\n",
    "        c stands for the cell state (memory)\n",
    "    \"\"\"\n",
    "\n",
    "    ## Retrieve parameters\n",
    "    # Forget gate\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "\n",
    "    # Update gate\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "\n",
    "    # Candidate value c~\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "\n",
    "    # Output gate\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "\n",
    "    # Prediction y_hat\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    ## Retrieve dimensions\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    ## Compute forward\n",
    "    # Concatenate a_prev and xt, shape (n_a + n_x, m)\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "\n",
    "    # Forget gate, shape (n_a, 1)\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "\n",
    "    # Update gate, shape (n_a, 1)\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "\n",
    "    # Candidate value, shape (n_a, 1)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "\n",
    "    # Output gate\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "\n",
    "    # Cell state\n",
    "    c_next = ft*c_prev + it*cct\n",
    "\n",
    "    # Hidden state\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    \n",
    "    # prediction\n",
    "    yt_hat = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    ## Cache + return\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "    return a_next, c_next, yt_hat, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward Pass\n",
    "\n",
    "<img src=\"./assets/LSTM_forwardpass.png\" width=\"1100\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(X, a0, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X (ndarray (n_x, m, T_x)) : Input data for every time-step\n",
    "        a0 (ndarray (n_a, m)) : Initial hidden state\n",
    "        parameters (dict) :\n",
    "            Wf (ndarray (n_a, n_a + n_x))    : Weight matrix of the forget gate\n",
    "            bf (ndarray (n_a, 1))            : Bias of the forget gate\n",
    "            Wi (ndarray (n_a, n_a + n_x))    : Weight matrix of the update gate\n",
    "            bi (ndarray (n_a, 1))            : Bias of the update gate\n",
    "            Wc (ndarray (n_a, n_a + n_x))    : Weight matrix of c~\n",
    "            bc (ndarray (n_a, 1))            : Bias of c~\n",
    "            Wo (ndarray (n_a, n_a + n_x))    : Weight matrix of the output gate\n",
    "            bo (ndarray (n_a, 1))            : Bias of the output gate\n",
    "            Wy (ndarray (n_y, n_a))          : Weight matrix of prediction y_hat\n",
    "            by (ndarray (n_y, 1))            : Bias of prediction y_hat\n",
    "\n",
    "    Returns:\n",
    "        a (ndarray (n_a, m, T_x))        : Hidden states for every time-step\n",
    "        Y_hat (ndarray (n_y, m, T_x))    : Predictions for every time-step\n",
    "        caches ((list(cache_t), X))      : list of cache every timestep t, X for the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dim\n",
    "    n_x, m, T_x = X.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # Initialize \"a\", \"c\" and \"Y_hat\"\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    Y_hat = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize at and ct\n",
    "    at = a0\n",
    "    ct = np.zeros(at.shape)\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    caches = []\n",
    "    for t in range(T_x):\n",
    "        # Cell forward\n",
    "        at, ct, yt, cache = lstm_cell_forward(X[:,:,t], at, ct, parameters)\n",
    "\n",
    "        # Update params_t\n",
    "        a[:,:,t] = at\n",
    "        c[:,:,t]  = ct\n",
    "        Y_hat[:,:,t] = yt\n",
    "\n",
    "        # Append the cache into caches\n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, X)\n",
    "    return a, Y_hat, c, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backpropagation\n",
    "\n",
    "## 2.1 LSTM backward Cell\n",
    "\n",
    "####  Gate derivatives\n",
    "\n",
    "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
    "\n",
    "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
    "\n",
    "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
    "\n",
    "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
    "\n",
    "#### Parameter derivatives \n",
    "\n",
    "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
    "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
    "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
    "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
    "\n",
    "#### Parameter derivatives \n",
    "\n",
    "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
    "\n",
    "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
    "\n",
    "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        da_next (ndarray (n_a, m)) : Gradient of loss with respect to next hidden state\n",
    "        dc_next (ndarray (n_a, m)) : Gradients of next cell state\n",
    "        cache (tuple) : contain output of the LSTM cell forward pass\n",
    "\n",
    "    Returns:\n",
    "        gradients (dict) :\n",
    "            dxt (ndarray (n_x, m))      : Gradients of input data\n",
    "            da_prev (ndarray (n_a, m))  : Gradients of previous hidden state\n",
    "            dc_prev (ndarray (n_a, m))  : Gradient of the previous memory state\n",
    "            dWf (ndarray (n_a, n_a + n_x))      : Gradient of the forget gate weight matrix\n",
    "            dWi (ndarray (n_a, n_a + n_x))      : Gradient of the update gate weight matrix\n",
    "            dWc (ndarray (n_a, n_a + n_x))      : Gradient of the memory gate weight matrix\n",
    "            dWo (ndarray (n_a, n_a + n_x))      : Gradient of the output gate weight matrix\n",
    "            dbf (ndarray (n_a, 1))              : Gradient of the forget gate bias matrix\n",
    "            dbi (ndarray (n_a, 1))              : Gradient of the update gate bias matrix\n",
    "            dbc (ndarray (n_a, 1))              : Gradient of the memory gate bias matrix\n",
    "            dbo (ndarray (n_a, 1))              : Gradient of the output gate bias matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from cache\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve dim\n",
    "    n_x, m = xt.shape \n",
    "    n_a, m = a_next.shape \n",
    "    \n",
    "    # Compute Gate derivatives\n",
    "    dit = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * cct * (1 - it) * it\n",
    "    dft = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * c_prev * ft * (1 - ft)\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * it * (1 - cct ** 2)\n",
    "\n",
    "    # Compute Parameters derivatives\n",
    "    dWf = np.dot(dft, np.concatenate((a_prev, xt), axis=0).T) # or use np.dot(dft, np.hstack([a_prev.T, xt.T]))\n",
    "    dWi = np.dot(dit, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWc = np.dot(dcct, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
    "    dbi = np.sum(dit, axis=1, keepdims=True) \n",
    "    dbc = np.sum(dcct, axis=1,keepdims=True) \n",
    "    dbo = np.sum(dot, axis=1, keepdims=True)  \n",
    "\n",
    "    # Compute  previous hidden state derivatives\n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft) \\\n",
    "        + np.dot(parameters['Wi'][:,:n_a].T,dit) \\\n",
    "        + np.dot(parameters['Wc'][:,:n_a].T,dcct) \\\n",
    "        + np.dot(parameters['Wo'][:,:n_a].T,dot) \n",
    "    dc_prev = dc_next * ft \\\n",
    "        + ot * (1-np.square(np.tanh(c_next))) * ft * da_next \n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft) \\\n",
    "        + np.dot(parameters['Wi'][:,n_a:].T,dit) \\\n",
    "        + np.dot(parameters['Wc'][:,n_a:].T,dcct) \\\n",
    "        + np.dot(parameters['Wo'][:,n_a:].T,dot) \n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\n",
    "        \"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev,\n",
    "        \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi, \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LSTM backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da (ndarray (n_a, m, T_x)) : Gradient of the hidden state\n",
    "    cache (list of tuples) : contain output of the LSTM forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients (dict) :\n",
    "            dxt (ndarray (n_x, m))      : Gradients of input data\n",
    "            da_prev (ndarray (n_a, m))  : Gradients of previous hidden state\n",
    "            dc_prev (ndarray (n_a, m))  : Gradient of the previous memory state\n",
    "            dWf (ndarray (n_a, n_a + n_x))      : Gradient of the forget gate weight matrix\n",
    "            dWi (ndarray (n_a, n_a + n_x))      : Gradient of the update gate weight matrix\n",
    "            dWc (ndarray (n_a, n_a + n_x))      : Gradient of the memory gate weight matrix\n",
    "            dWo (ndarray (n_a, n_a + n_x))      : Gradient of the output gate weight matrix\n",
    "            dbf (ndarray (n_a, 1))              : Gradient of the forget gate bias matrix\n",
    "            dbi (ndarray (n_a, 1))              : Gradient of the update gate bias matrix\n",
    "            dbc (ndarray (n_a, 1))              : Gradient of the memory gate bias matrix\n",
    "            dbo (ndarray (n_a, 1))              : Gradient of the output gate bias matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1)\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dim\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # zero gradients\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWi = np.zeros((n_a, n_a + n_x))\n",
    "    dWc = np.zeros((n_a, n_a + n_x))\n",
    "    dWo = np.zeros((n_a, n_a + n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))\n",
    "    \n",
    "    # loop back over all timesteps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradient timestep t\n",
    "        gradients = lstm_cell_backward(da[:,:,t] + da_prevt, dc_prevt, caches[t])\n",
    "\n",
    "        # Update gradient at timestep t\n",
    "        dx[:,:,t] = gradients[\"dxt\"]\n",
    "        dWf += gradients[\"dWf\"]\n",
    "        dWi += gradients[\"dWi\"]\n",
    "        dWc += gradients[\"dWc\"]\n",
    "        dWo += gradients[\"dWo\"]\n",
    "        dbf += gradients[\"dbf\"]\n",
    "        dbi += gradients[\"dbi\"]\n",
    "        dbc += gradients[\"dbc\"]\n",
    "        dbo += gradients[\"dbo\"]\n",
    "\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = gradients[\"da_prev\"]\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
