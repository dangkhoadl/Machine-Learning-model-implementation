{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SimCLR\n",
    "- **Simple Framework for Contrastive Learning of Visual Representations** is a State-of-the-art Self-supervised Representation Learning Framework.\n",
    "\n",
    "#### [Papers](https://arxiv.org/pdf/2002.05709.pdf)\n",
    "- Unsupervised representation learning benefits from **strong augmentations**.\n",
    "- Introducing a trainable MLP after the base encoder\n",
    "    + improves the quality of the learned representations.\n",
    "- Representation learning with contrastive cross-entropy loss benefits from\n",
    "    + normalized embeddings\n",
    "    + and an adjusted temperature parameter.\n",
    "- Contrastive learning benefits from\n",
    "    + larger batch sizes\n",
    "    + and longer training compared to its supervised counterpart.\n",
    "- Like supervised learning, contrastive learning benefits from deeper and wider networks.\n",
    "\n",
    "#### SimCLR framework\n",
    "- **A stochastic data augmentation** module\n",
    "    + Transforms any given data example randomly $\\to$ 2 correlated views of the same example.\n",
    "    + This pair is considered to be a positive pair.\n",
    "- **A base encoder** $f(.)$\n",
    "    + Extracts representation vectors from augmented data examples.\n",
    "- **A small neural network projection head** $g(.)$\n",
    "    + maps representations to space where contrastive loss is applied.\n",
    "- **A Contrastive Loss function** defined for a contrastive prediction task.\n",
    "\n",
    "<img src=\"assets/simCLR.png\"  width=\"500\">\n",
    "\n",
    "#### Contrastive loss function\n",
    "\n",
    "<img src=\"assets/loss.png\"  width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
