{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Binary Cross Entropy (Log Loss)\n",
    "## 1.1 Definition\n",
    "\n",
    "\n",
    "#### Binary Cross Entropy - Log likelihood\n",
    "- $y_i = $ `0` or `1` (True, False)\n",
    "- $p_i = $ probability of being True, in range `[0,1]`\n",
    "- **Low CE = better prediction**\n",
    "\n",
    "$$\\text{CE} = - \\sum\\limits_{i=1}^N \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)  \\right] $$\n",
    "\n",
    "\n",
    "#### Example 1\n",
    "+ Given a classifier predicts the probability of 3 box that have gifts inside (Yes/No)\n",
    "+ Calculate the cross-entropy\n",
    "\n",
    "| x                               | **Box 1** | **Box 2** | **Box 3** |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "| Predict (p)                     | 0.8       | 0.7       | 0.1       |\n",
    "| Actual value (have gift inside) | Yes       | Yes       | No        |\n",
    "\n",
    "- Solve\n",
    "\n",
    "$$\\begin{split}\n",
    "CE &= - \\left[  (1*log(0.8)+0*log(1-0.8)) + (1*log(0.7)+0*log(1-0.7)) + (0*(0.1)+1*log(1-0.1)) \\right] \\\\\n",
    "    &= 0.69\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "#### Example 2\n",
    "+ Given a classifier predicts the probability of 3 box that have gifts inside (Yes/No)\n",
    "+ Calculate the cross-entropy\n",
    "\n",
    "| x                               | **Box 1** | **Box 2** | **Box 3** |\n",
    "|---------------------------------|-----------|-----------|-----------|\n",
    "| Predict (p)                     | 0.8       | 0.7       | 0.1       |\n",
    "| Actual value (have gift inside) | No        | No        | Yes       |\n",
    "\n",
    "- Solve\n",
    "\n",
    "$$\\begin{split}\n",
    "CE &= \\left[  (0*log(0.8)+1*log(1-0.8)) + (0*log(0.7)+1*log(1-0.7)) + (1*log(0.1)+0*log(1-0.1)) \\right] \\\\\n",
    "    &= 5.12\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "\n",
    "## 1.2 CE as Loss function\n",
    "- Applied to Logistic Regression\n",
    "- logistic function = `softmax` to simulate the output probability \n",
    "\n",
    "\n",
    "<img src=\"./assets/2.svg\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Binary Classification\n",
    "- $y_i$: correct label `0` or `1`\n",
    "- $\\hat{y}_i$: output of logistic function in range `[0,1]`\n",
    "\n",
    "\n",
    "$$L(\\hat{y}_i, y_i) = - \\sum\\limits_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-class Cross Entropy\n",
    "\n",
    "## 2.1 Definition\n",
    "\n",
    "$$CE = - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^M \\left[ y_{ij} * log(p_{ij})  \\right] =  - \\sum\\limits_{i=1}^N \\left[ y_{i} * log(p_{i})  \\right]$$\n",
    "\n",
    "\n",
    "#### Example\n",
    "+ Given a classifier predicts the probability of 3 rooms that have which animal inside (Duck/Bear/Seal)\n",
    "+ Calculate the cross-entropy\n",
    "\n",
    "| x            | **Room 1** | **Room 2** | **Room 3** |\n",
    "|--------------|------------|------------|------------|\n",
    "| Predict Duck | 0.7        | 0.3        | 0.1        |\n",
    "| Predict Bear | 0.2        | 0.4        | 0.5        |\n",
    "| Predict Seal | 0.1        | 0.3        | 0.4        |\n",
    "| Actual value | Duck       | Seal       | Seal       |\n",
    "\n",
    "- Solve\n",
    "\n",
    "$$\\begin{split}\n",
    "CE &= - \\left[ (1*log(0.7)+0*log(0.2)+0*log(0.1) + (0*log(0.3)+0*log(0.4)+1*log(0.3) + (0*log(0.1)+0*log(0.5)+1*log(0.4) \\right] \\\\\n",
    "    &= 2.48\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Multi-label Classification - Loss Function\n",
    "\n",
    "- $\\hat{y}_{ij}$: output of logistic function from j-class $\\in [0,1]$\n",
    "- $y_{ij}$: correct label of j-class `0` or `1`\n",
    "\n",
    "$$L = \\frac{-1}{N} \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^M y_{ij} * log(\\hat{y}_{ij}) = \\frac{-1}{N} \\sum\\limits_{i=1}^N y_{i} * log(\\hat{y}_{i})$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
