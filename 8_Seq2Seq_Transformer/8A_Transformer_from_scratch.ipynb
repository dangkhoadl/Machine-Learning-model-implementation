{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source sequence X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chars = [chr(asc_idx) for asc_idx in range(ord('a'), ord('z')+1)] \\\n",
    "    + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "\n",
    "X_lexicon = { ch:idx for idx, ch in enumerate(X_chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size() = torch.Size([16, 50])\n",
      "X[0] = tensor([ 3, 19, 12, 18,  9, 20, 10,  6,  8, 10,  4, 23, 13,  6, 10, 18, 22, 11,\n",
      "        18, 12, 27,  6, 15, 23, 19,  4, 28, 21, 18,  5,  3,  6,  0, 16, 15,  5,\n",
      "         5, 26, 25, 29, 15, 21, 16, 28,  0, 17, 27,  9, 16, 29])\n"
     ]
    }
   ],
   "source": [
    "m = 16\n",
    "Tx = 50\n",
    "X_lexicon_size = len(X_lexicon) # 30\n",
    "\n",
    "# (m, Tx)\n",
    "X = torch.randint(low=0, high=X_lexicon_size, size=(m, Tx))\n",
    "print(f'{X.size() = }')\n",
    "print(f'{X[0] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target sequence Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_chars = [chr(asc_idx) for asc_idx in range(ord('0'), ord('9')+1)] \\\n",
    "    + ['<unk>', '<pad>', '<start>', '<end>']\n",
    "Y_lexicon = { ch:idx for idx, ch in enumerate(Y_chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.size() = torch.Size([16, 40])\n",
      "Y[0] = tensor([ 5,  3,  0, 10,  2,  5,  9, 13, 10, 11,  7,  9, 11,  7, 10,  7,  3,  0,\n",
      "        11,  2,  0,  4,  6,  4,  2,  4, 10, 12,  8,  5,  8,  0,  0,  2,  7, 10,\n",
      "         0,  3,  3, 10])\n"
     ]
    }
   ],
   "source": [
    "m = 16\n",
    "Ty = 40\n",
    "Y_lexicon_size = len(Y_lexicon) # 14\n",
    "\n",
    "# (m, Ty)\n",
    "Y = torch.randint(low=0, high=Y_lexicon_size, size=(m, Ty))\n",
    "print(f'{Y.size() = }')\n",
    "print(f'{Y[0] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Components\n",
    "[https://arxiv.org/pdf/1706.03762v5.pdf](https://arxiv.org/pdf/1706.03762v5.pdf)\n",
    "\n",
    "<img src=\"assets/transformer_transformer.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Embedding\n",
    "- Encode **word meaning info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embbeding(nn.Module):\n",
    "    def __init__(self, lexicon_size, embed_dim):\n",
    "        super(Embbeding, self).__init__()\n",
    "\n",
    "        # Emb\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=lexicon_size,\n",
    "            embedding_dim=embed_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            inp (Long tensor(m, T))         : Input sequence, categorical Long\n",
    "                m : batch size\n",
    "                T : sequence length\n",
    "        Returns:\n",
    "            emb (tensor(m, T, embed_dim))   : Embbeding sequence\n",
    "        \"\"\"\n",
    "        # (m, T) -> (m, T, emb_dim)\n",
    "        return self.embedding(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_emb.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "embed_x = Embbeding(\n",
    "    lexicon_size=X_lexicon_size,\n",
    "    embed_dim=64)\n",
    "X_emb = embed_x(X)\n",
    "\n",
    "# (m, Tx, emb_dim)\n",
    "print(f'{X_emb.size() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_emb.size() = torch.Size([16, 40, 64])\n"
     ]
    }
   ],
   "source": [
    "embed_y = Embbeding(\n",
    "    lexicon_size=Y_lexicon_size,\n",
    "    embed_dim=64)\n",
    "Y_emb = embed_y(Y)\n",
    "\n",
    "# (m, Ty, emb_dim)\n",
    "print(f'{Y_emb.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Positional Encoder\n",
    "- Encode **word positioning info** (pos of a word in a sentence)\n",
    "- PE = a matrix size (Tx, emb_dim)\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\text{PE(pos, 2i)} = \\sin \\left(\\frac{\\text{pos}}{10000^{2i / d}} \\right) \\text{ (even i)} \\\\\n",
    "\\text{PE(pos, 2i + 1)} = \\cos \\left(\\frac{\\text{pos}}{10000^{2i / d}} \\right) \\text{ (odd i)}\n",
    "\\end{cases}$$\n",
    "\n",
    "- X_embed_pe = A*embed + PE\n",
    "    + Amplify embed values to prevent embedding info supressed by PE info after adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "T, emb_dim = 100, 64\n",
    "\n",
    "# Init PE matrix\n",
    "#    (T, emb_dim)\n",
    "pe = np.zeros((T, emb_dim))\n",
    "\n",
    "# Assign pe values\n",
    "for t in range(T):\n",
    "    for i in range(0, emb_dim, 2):\n",
    "        pe[t, i] = np.sin(\n",
    "            t / (10000 ** ((2 * i) / emb_dim)))\n",
    "        pe[t, i+1] = np.cos(\n",
    "            t / (10000 ** ((2 * (i + 1)) / emb_dim)))\n",
    "        \n",
    "# Plot\n",
    "sns.heatmap(pe, cmap=plt.cm.Reds)\n",
    "plt.xlabel('Embbed dim')\n",
    "plt.ylabel('Position (t)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, T, embed_dim):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.emb_dim = embed_dim\n",
    "\n",
    "        # Init PE matrix\n",
    "        #    (T, emb_dim)\n",
    "        pe = torch.zeros(T, self.emb_dim)\n",
    "\n",
    "        # Assign pe values\n",
    "        for t in range(T):\n",
    "            for i in range(0, self.emb_dim, 2):\n",
    "                pe[t, i] = np.sin(\n",
    "                    t / (10000 ** ((2 * i) / self.emb_dim)))\n",
    "                pe[t, i+1] = np.cos(\n",
    "                    t / (10000 ** ((2 * (i + 1)) / self.emb_dim)))\n",
    "\n",
    "        # (T, emb_dim) -> (1, T, emb_dim)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        Add position encoding info to emb\n",
    "\n",
    "        Arguments:\n",
    "            emb (tensor(m, T, embed_dim))       : Embbeding sequence\n",
    "\n",
    "        Returns:\n",
    "            emb_pos (tensor(m, T, embed_dim))   : Embbeding + PE sequence\n",
    "        \"\"\"\n",
    "        # Get dim\n",
    "        T = emb.size(1)\n",
    "        \n",
    "        # Retrieve pe\n",
    "        pe = Variable(self.pe[:,:T], requires_grad=False)\n",
    "        if emb.is_cuda: pe.cuda()\n",
    "\n",
    "        # emb_pos = rescaled*emb + position encoded\n",
    "        emb_pos = np.sqrt(self.emb_dim)*emb + pe\n",
    "        return emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_emb_pos.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "pos_encoder_x = PositionalEncoder(\n",
    "    T=Tx,\n",
    "    embed_dim=64)\n",
    "\n",
    "X_emb_pos = pos_encoder_x(X_emb)\n",
    "print(f'{X_emb_pos.size() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_emb_pos.size() = torch.Size([16, 40, 64])\n"
     ]
    }
   ],
   "source": [
    "pos_encoder_y = PositionalEncoder(\n",
    "    T=Ty,\n",
    "    embed_dim=64)\n",
    "\n",
    "Y_emb_pos = pos_encoder_y(Y_emb)\n",
    "print(f'{Y_emb_pos.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multi-Headed Attention\n",
    "\n",
    "#### Masking\n",
    "- **pad token mask**: Suppress attention where `<pad>` exists\n",
    "- **No peaking forward Mask**: For Target sequence, prevent the decoder paying attention ahead toward the rest of the translated sentence when predicting the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source mask example\n",
    "\n",
    "```python\n",
    "# X_seq = (3, 14), pad_token = 0\n",
    "X_seq = tensor([\n",
    "    [ 101, 1045, 2123, 1005, 1056, 2113, 1998, 1045, 2123, 1005, 1056, 2729, 1012,  102],\n",
    "    [ 101, 2054, 2395, 2079, 2017, 2444, 2006, 1029,  102,    0,    0,    0, 0,    0],\n",
    "    [ 101, 3419, 5598, 2046, 1996, 4770, 1012,  102,    0,    0,    0,    0, 0,    0]])\n",
    "\n",
    "# X_mask = (3, 1, 1, 14)\n",
    "X_mask = tensor([\n",
    "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
    "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]],\n",
    "        [[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]]], dtype=torch.int8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_source(X_seq, pad_token, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create mask values (0 = masked, 1 = keep)\n",
    "\n",
    "    Arguments:\n",
    "        X_seq (Long tensor(m, T)) : Source Sequence, categorical Long\n",
    "            m : batch size\n",
    "            T : sequence length\n",
    "        pad_token (int)           : token value of <pad>, get from X_lexicon['<pad>']\n",
    "\n",
    "    Returns:\n",
    "        mask (Long tensor(m, 1, 1, T)) : attention mask(val=1:keep or val=0:supress) for source sequence\n",
    "    \"\"\"\n",
    "    # Get dim\n",
    "    m, Tx = X_seq.size()\n",
    "\n",
    "    # padding token Mask\n",
    "    #    mask = 1 if seq != padding token else 0\n",
    "    #    (m, T) -> (m, 1, 1, T)\n",
    "    padding_mask = (X_seq != pad_token) \\\n",
    "        .unsqueeze(dim=1).unsqueeze(dim=2) \\\n",
    "        .to(device)\n",
    "\n",
    "    return padding_mask \\\n",
    "        .to(torch.int8).requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mask.size() = torch.Size([16, 1, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "X_mask = create_mask_source(X, pad_token=X_lexicon['<pad>'])\n",
    "print(f'{X_mask.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target mask example\n",
    "\n",
    "```python\n",
    "# Y_seq = (3, 12), pad_token = 1\n",
    "Y_seq = tensor([\n",
    "    [0, 70, 17, 55, 6, 70, 17,  2665, 25349, 21573, 2, 1],\n",
    "    [0, 88, 235, 25, 109, 142, 2, 1, 1, 1, 1, 1],\n",
    "    [0, 33939, 1945, 307, 1005, 1388, 9479, 27375, 2, 1, 1, 1]])\n",
    "\n",
    "# Y_mask = (3, 1, 12, 12)\n",
    "Y_mask = tensor([[\n",
    "         [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
    "\n",
    "        [[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]],\n",
    "\n",
    "        [[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]]], dtype=torch.int8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_target(Y_seq, pad_token, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create mask values (0 = masked, 1 = keep)\n",
    "\n",
    "    Arguments:\n",
    "        Y_seq (Long tensor(m, T)) : Target Sequence, categorical Long\n",
    "            m : batch size\n",
    "            T : sequence length\n",
    "        pad_token (int)           : token value of <pad>, get from Y_lexicon['<pad>']\n",
    "\n",
    "    Returns:\n",
    "        mask (Long tensor(m, 1, T, T)) : attention mask(val=1:keep or val=0:supress) for target sequence\n",
    "    \"\"\"\n",
    "    # Get dim\n",
    "    m, Ty = Y_seq.size()\n",
    "\n",
    "    # padding token Mask\n",
    "    #    mask = 1 if seq != padding token else 0\n",
    "    #    (m, T) -> (m, 1, 1, T) -> (m, 1, T, T)\n",
    "    padding_mask = (Y_seq != pad_token) \\\n",
    "        .unsqueeze(dim=1).unsqueeze(dim=2) \\\n",
    "        .expand(m, 1, Ty, Ty).to(torch.bool) \\\n",
    "        .to(device)\n",
    "\n",
    "    # No peaking forward Mask: (m, 1, T, T)\n",
    "    #   (T, T) = Lower triangular matrix\n",
    "    nopeak_mask = torch.tril(torch.ones( (Ty, Ty) )) \\\n",
    "        .unsqueeze(dim=0).unsqueeze(dim=1) \\\n",
    "        .expand(m, 1, Ty, Ty).to(torch.bool) \\\n",
    "        .to(device)\n",
    "\n",
    "    return (padding_mask & nopeak_mask) \\\n",
    "        .to(torch.int8).requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_mask.size() = torch.Size([16, 1, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "Y_mask = create_mask_target(Y, pad_token=Y_lexicon['<pad>'])\n",
    "print(f'{Y_mask.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$$\n",
    "\n",
    "<img src=\"assets/transformer_SDPAtt.png\" width=\"200\">\n",
    "\n",
    "#### Multi-head Attention\n",
    "\n",
    "<img src=\"assets/transformer_multiheadAtt.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Params\n",
    "        self.emb_dim = embed_dim\n",
    "        self.h = num_heads\n",
    "\n",
    "        # Split emb_dim = num_head * head_dim\n",
    "        self.head_dim = self.emb_dim // self.h\n",
    "\n",
    "        assert (\n",
    "            self.emb_dim == self.h * self.head_dim\n",
    "        ), \"Embedding size needs to be divisible by num_heads\"\n",
    "\n",
    "        # Linear key, queries, values\n",
    "        self.q_linears = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.k_linears = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.v_linears = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(num_heads*self.head_dim, self.emb_dim)\n",
    "\n",
    "    def __scaled_dot_product_att(self, q, k, v, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot product attention\n",
    "\n",
    "        Arguments:\n",
    "            q (tensor(m, T_query, num_heads, head_dim))        : query\n",
    "            k (tensor(m, T_key, num_heads, head_dim))          : key\n",
    "            v (tensor(m, T_val, num_heads, head_dim))          : value\n",
    "            mask (tensor None or (m, 1, T, T) or (m, 1, 1, T)) : attention mask, 1=keep, 0=suppress att\n",
    "            dropout (None or nn.Dropout)                       : dropout function\n",
    "        Returns:\n",
    "            attention (tensor(m, T_query, num_heads, head_dim)): scaled dot product attention\n",
    "        \"\"\"\n",
    "        # energy = (m, T_query, num_heads, head_dim) x (m, T_key, num_heads, head_dim)\n",
    "        #    -> (m, num_heads, T_query, T_key)\n",
    "        energy = torch.einsum(\"mqhd,mkhd->mhqk\", [q, k]) / np.sqrt(self.emb_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Masking, suppress energy/attention where mask == 0\n",
    "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "        # softmax = (m, num_heads, T_query, T_key)\n",
    "        energy_sm = F.softmax(energy, dim=3)\n",
    "        if dropout is not None:\n",
    "            energy_sm = dropout(energy_sm)\n",
    "\n",
    "        # attention = softmax * v\n",
    "        #   (m, T_query, num_heads, head_dim)\n",
    "        attention = torch.einsum(\"mhqk,mvhd->mqhd\", [energy_sm, v])\n",
    "        return attention\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute Multihead attention\n",
    "\n",
    "        Arguments:\n",
    "            Q (tensor(m, T_query, emb_dim))        : Query\n",
    "            K (tensor(m, T_key, emb_dim))          : Key\n",
    "            V (tensor(m, T_val, emb_dim))          : Value\n",
    "            mask (tensor None or (m, 1, T, T) or (m, 1, 1, T)) : attention mask, 1=keep, 0=suppress att\n",
    "        Returns:\n",
    "            attention (tensor(m, T_query, emb_dim)): multihead attention\n",
    "        \"\"\"\n",
    "\n",
    "        # Batch size\n",
    "        m = Q.size(0)\n",
    "\n",
    "        # Compute key, queries, values linears, then split q,k,v -> multiple heads\n",
    "        #   (m, T_query, emb_dim) -> (m, T_query, num_heads, head_dim)\n",
    "        #   (m, T_key, emb_dim) -> (m, T_key, num_heads, head_dim)\n",
    "        #   (m, T_val, emb_dim) -> (m, T_val, num_heads, head_dim)\n",
    "        queries = self.q_linears(Q).view(m, -1, self.h, self.head_dim)\n",
    "        keys = self.k_linears(K).view(m, -1, self.h, self.head_dim)\n",
    "        values = self.v_linears(V).view(m, -1, self.h, self.head_dim)\n",
    "\n",
    "        # Compute scaled dot product attention\n",
    "        #    (m, T_query, num_heads, head_dim)\n",
    "        attention = self.__scaled_dot_product_att(\n",
    "            q=queries, k=keys, v=values,\n",
    "            mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # concatenate all heads\n",
    "        #    (m, T_query, emb_dim)\n",
    "        concat = attention.view(m, -1, self.h * self.head_dim)\n",
    "\n",
    "        # Compute last linear\n",
    "        attention = self.linear(concat)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_self_att.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "self_att_x = MultiHeadAttention(embed_dim=64, num_heads=4, dropout=0.1)\n",
    "\n",
    "X_self_att = self_att_x(Q=X_emb_pos, K=X_emb_pos, V=X_emb_pos, mask=X_mask)\n",
    "print(f'{X_self_att.size() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_self_att.size() = torch.Size([16, 40, 64])\n"
     ]
    }
   ],
   "source": [
    "self_att_y = MultiHeadAttention(embed_dim=64, num_heads=4, dropout=0.1)\n",
    "\n",
    "Y_self_att = self_att_y(Q=Y_emb_pos, K=Y_emb_pos, V=Y_emb_pos, mask=Y_mask)\n",
    "print(f'{Y_self_att.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Layer Norm\n",
    "- Perform Layer Normalization across each example, agg `feature dimensions`\n",
    "    + $\\gamma$: weight parameter, grad = True\n",
    "    + $\\beta$: bias parameter, grad = True\n",
    "\n",
    "$$y  = \\frac{x - E(x)}{\\sqrt{\\text{Var}(x) + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # Trainable params\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape), requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute Norm across each example, agg (emb_dim), same as nn.LayerNorm(emb_dim)\n",
    "\n",
    "        Arguments:\n",
    "            x (tensor(m, T, emb_dim))  : Input\n",
    "            affine (bool)              : If True apply transform to input x\n",
    "        Returns:\n",
    "            y (tensor(m, T, emb_dim))  : Norm output\n",
    "        \"\"\"\n",
    "        E_x = x.mean(dim=-1, keepdim=True)\n",
    "        Var_x = torch.var(x, dim=-1, keepdim=True)\n",
    "        return (x - E_x) / torch.sqrt(Var_x + self.eps) * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_norm.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 64\n",
    "\n",
    "norm = LayerNorm(normalized_shape=emb_dim, eps=1e-6)\n",
    "\n",
    "X_norm = norm(X_self_att)\n",
    "print(f'{X_norm.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, forward_expansion_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(forward_expansion_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Apply feed forward toward last dimension\n",
    "\n",
    "        Arguments:\n",
    "            inp (tensor(m,*,embed_dim))  : Input tensor\n",
    "        Returns:\n",
    "            out (tensor(m,*,embed_dim))  : Output tensor\n",
    "        \"\"\"\n",
    "        return self.feed_forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "feedforward = FeedForward(embed_dim=64, forward_expansion_dim=2048, dropout=0.1)\n",
    "\n",
    "ff = feedforward(X_norm)\n",
    "print(f'{ff.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoder\n",
    "### 2.1 Encoder Block\n",
    "\n",
    "<img src=\"assets/transformer_blk_encoder.png\" width=\"150\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim, num_heads,\n",
    "            dropout=0.1, forward_expansion_dim=2048, eps=1e-5):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.multihead_attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads,\n",
    "            dropout=dropout)\n",
    "        self.norm_1 = LayerNorm(\n",
    "            normalized_shape=embed_dim,\n",
    "            eps=eps)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer 2\n",
    "        self.feed_forward = FeedForward(\n",
    "            embed_dim=embed_dim, forward_expansion_dim=forward_expansion_dim,\n",
    "            dropout=dropout)\n",
    "        self.norm_2 =LayerNorm(\n",
    "            normalized_shape=embed_dim,\n",
    "            eps=eps)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X_emb_pos, X_mask):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X_emb_pos (tensor(m, Tx, emb_dim) : Source sequence encoded with embbeding and position\n",
    "            X_mask (Long tensor(m, 1, 1, T))  : Source sequence attention mask(val=1:keep or val=0:supress)\n",
    "        Returns:\n",
    "            X_enc (tensor(m, Tx, emb_dim))    : encoder output from source seq\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        att = self.multihead_attention(\n",
    "            Q=X_emb_pos, K=X_emb_pos, V=X_emb_pos,\n",
    "            mask=X_mask)\n",
    "        X_norm_1 = self.dropout_1(\n",
    "            self.norm_1(X_emb_pos + att))\n",
    "\n",
    "        # Layer 2\n",
    "        ff = self.feed_forward(X_norm_1)\n",
    "        X_enc = self.dropout_2(\n",
    "            self.norm_2(X_norm_1 + ff))\n",
    "\n",
    "        return X_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder\n",
    "\n",
    "<img src=\"assets/transformer_encoder.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "            Tx, X_lexicon_size, embed_dim,\n",
    "            num_layers, num_heads,\n",
    "            forward_expansion_dim=1024,\n",
    "            dropout=0.1, eps=1e-5):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Params\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # X = emb + pe\n",
    "        self.input_embedding = Embbeding(\n",
    "            lexicon_size=X_lexicon_size, embed_dim=embed_dim)\n",
    "        self.pos_encoding = PositionalEncoder(\n",
    "            T=Tx, embed_dim=embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Enc blocks\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                embed_dim=embed_dim, num_heads=num_heads,\n",
    "                dropout=dropout, forward_expansion_dim=forward_expansion_dim, eps=eps\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X_seq, X_mask):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X_seq (Long tensor(m, Tx)         : Source sequence, categorical Long\n",
    "            X_mask (Long tensor(m, 1, 1, T))  : Source sequence attention mask(val=1:keep or val=0:supress)\n",
    "        Returns:\n",
    "            X_enc (tensor(m, Tx, emb_dim))    : encoder output from source seq\n",
    "        \"\"\"\n",
    "        # X = emb + pe\n",
    "        X_emb = self.input_embedding(X_seq)\n",
    "        X_emb_pos = self.pos_encoding(X_emb)\n",
    "        X_enc = self.dropout(X_emb_pos)\n",
    "\n",
    "        # Enc blocks\n",
    "        for enc_blk in self.enc_blocks:\n",
    "            X_enc = enc_blk(\n",
    "                X_emb_pos=X_enc, X_mask=X_mask)\n",
    "\n",
    "        return X_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_enc.size() = torch.Size([16, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(\n",
    "    Tx=Tx, X_lexicon_size=X_lexicon_size, embed_dim=64,\n",
    "    num_layers=3, num_heads=4,\n",
    "    forward_expansion_dim=1024,\n",
    "    dropout=0.1, eps=1e-6)\n",
    "\n",
    "X_enc = encoder(X_seq=X, X_mask=X_mask)\n",
    "print(f'{X_enc.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decoder\n",
    "### 3.1 Decoder Block\n",
    "\n",
    "<img src=\"assets/transformer_blk_decoder.png\" width=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim, num_heads,\n",
    "            dropout=0.1, forward_expansion_dim=2048, eps=1e-5):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # Layer 1\n",
    "        self.masked_multihead_attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads,\n",
    "            dropout=dropout)\n",
    "        self.norm_1 = LayerNorm(\n",
    "            normalized_shape=embed_dim,\n",
    "            eps=eps)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer 2\n",
    "        self.multihead_attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads,\n",
    "            dropout=dropout)\n",
    "        self.norm_2 = LayerNorm(\n",
    "            normalized_shape=embed_dim,\n",
    "            eps=eps)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer 3\n",
    "        self.feed_forward = FeedForward(\n",
    "            embed_dim=embed_dim, forward_expansion_dim=forward_expansion_dim,\n",
    "            dropout=dropout)\n",
    "        self.norm_3 = LayerNorm(\n",
    "            normalized_shape=embed_dim,\n",
    "            eps=eps)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "            Y_emb_pos, Y_mask,\n",
    "            X_enc, X_mask):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            Y_emb_pos (tensor(m, Ty, emb_dim) : Target sequence encoded with embbeding and position\n",
    "            Y_mask (Long tensor(m, 1, T, T))  : Target sequence attention mask(val=1:keep or val=0:supress)\n",
    "            X_enc (tensor(m, Tx, emb_dim))    : encoder output from source seq\n",
    "            X_mask (Long tensor(m, 1, 1, T))  : Source sequence attention mask(val=1:keep or val=0:supress)\n",
    "        Returns:\n",
    "            Y_dec (tensor(m, Ty, emb_dim))     : deccoder output\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        masked_att = self.masked_multihead_attention(\n",
    "            Q=Y_emb_pos, K=Y_emb_pos, V=Y_emb_pos,\n",
    "            mask=Y_mask)\n",
    "        Y_norm_1 = self.dropout_1(\n",
    "            self.norm_1(Y_emb_pos + masked_att))\n",
    "\n",
    "        # Layer 2\n",
    "        att = self.multihead_attention(\n",
    "            Q=Y_norm_1, K=X_enc, V=X_enc,\n",
    "            mask=X_mask)\n",
    "        Y_norm_2 = self.dropout_2(\n",
    "            self.norm_2(Y_norm_1 + att))\n",
    "\n",
    "        # Layer 3\n",
    "        ff = self.feed_forward(Y_norm_2)\n",
    "        Y_dec = self.dropout_3(\n",
    "            self.norm_3(Y_norm_2 + ff))\n",
    "\n",
    "        return Y_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decoder\n",
    "\n",
    "<img src=\"assets/transformer_decoder.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "            Ty, Y_lexicon_size, embed_dim,\n",
    "            num_layers, num_heads,\n",
    "            forward_expansion_dim=1024,\n",
    "            dropout=0.1, eps=1e-5):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Params\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Y = emb + pe\n",
    "        self.input_embedding = Embbeding(\n",
    "            lexicon_size=Y_lexicon_size, embed_dim=embed_dim)\n",
    "        self.pos_encoding = PositionalEncoder(\n",
    "            T=Ty, embed_dim=embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Dec blocks\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "             DecoderBlock(\n",
    "                embed_dim=embed_dim, num_heads=num_heads,\n",
    "                dropout=dropout, forward_expansion_dim=forward_expansion_dim, eps=eps\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Classifier\n",
    "        self.fc_out = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=Y_lexicon_size)\n",
    "\n",
    "    def forward(self,\n",
    "            Y_seq, Y_mask,\n",
    "            X_enc, X_mask):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            Y_seq (Long tensor(m, Ty)         : Target sequence, categorical Long\n",
    "            Y_mask (Long tensor(m, 1, T, T))  : Target sequence attention mask(val=1:keep or val=0:supress)\n",
    "            X_enc (tensor(m, Tx, emb_dim))    : encoder output from source seq\n",
    "            X_mask (Long tensor(m, 1, 1, T))  : Source sequence attention mask(val=1:keep or val=0:supress)\n",
    "        Returns:\n",
    "            out (tensor(m, Ty, emb_dim))      : log softmax probability predict category in Y_lexicon\n",
    "        \"\"\"\n",
    "        # Y = emb + pe\n",
    "        Y_emb = self.input_embedding(Y_seq)\n",
    "        Y_emb_pos = self.pos_encoding(Y_emb)\n",
    "        Y_dec = self.dropout(Y_emb_pos)\n",
    "\n",
    "        # Dec blocks\n",
    "        for dec_blk in self.dec_blocks:\n",
    "            Y_dec = dec_blk(\n",
    "                Y_emb_pos=Y_dec, Y_mask=Y_mask,\n",
    "                X_enc=X_enc, X_mask=X_mask)\n",
    "\n",
    "        # Classifier\n",
    "        out = self.fc_out(Y_dec)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.size() = torch.Size([16, 40, 14])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(\n",
    "    Ty=Ty, Y_lexicon_size=Y_lexicon_size, embed_dim=64,\n",
    "    num_layers=3, num_heads=4,\n",
    "    forward_expansion_dim=1024,\n",
    "    dropout=0.1, eps=1e-6)\n",
    "\n",
    "out = decoder(\n",
    "    Y_seq=Y, Y_mask=Y_mask,\n",
    "    X_enc=X_enc, X_mask=X_mask)\n",
    "print(f'{out.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "            Tx, X_lexicon_size,\n",
    "            Ty, Y_lexicon_size,\n",
    "            embed_dim=512,\n",
    "            num_layers=3, num_heads=4,\n",
    "            forward_expansion_dim=1024,\n",
    "            dropout=0.1, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            Tx=Tx, X_lexicon_size=X_lexicon_size, embed_dim=embed_dim,\n",
    "            num_layers=num_layers, num_heads=num_heads,\n",
    "            forward_expansion_dim=forward_expansion_dim,\n",
    "            dropout=dropout, eps=eps)\n",
    "        self.decoder = Decoder(\n",
    "            Ty=Ty, Y_lexicon_size=Y_lexicon_size, embed_dim=embed_dim,\n",
    "            num_layers=num_layers, num_heads=num_heads,\n",
    "            forward_expansion_dim=forward_expansion_dim,\n",
    "            dropout=dropout, eps=eps)\n",
    "\n",
    "    def forward(self, X_seq, X_mask, Y_seq, Y_mask):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X_seq (Long tensor(m, Tx)         : Source sequence, categorical Long\n",
    "            X_mask (Long tensor(m, 1, 1, T))  : Source sequence attention mask(val=1:keep or val=0:supress)\n",
    "            Y_seq (Long tensor(m, Ty)         : Target sequence, categorical Long\n",
    "            Y_mask (Long tensor(m, 1, T, T))  : Target sequence attention mask(val=1:keep or val=0:supress)\n",
    "        Returns:\n",
    "            out (tensor(m, Ty, emb_dim))      : log softmax probability predict category in Y_lexicon\n",
    "        \"\"\"\n",
    "        X_enc = self.encoder(\n",
    "            X_seq=X_seq, X_mask=X_mask)\n",
    "        out = self.decoder(\n",
    "            Y_seq=Y_seq, Y_mask=Y_mask,\n",
    "            X_enc=X_enc, X_mask=X_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.size() = torch.Size([16, 40, 14])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    Tx=Tx, X_lexicon_size=X_lexicon_size,\n",
    "    Ty=Ty, Y_lexicon_size=Y_lexicon_size,\n",
    "    embed_dim=512,\n",
    "    num_layers=4, num_heads=4,\n",
    "    forward_expansion_dim=1024,\n",
    "    dropout=0.1, eps=1e-6)\n",
    "\n",
    "out = transformer(\n",
    "    X_seq=X, X_mask=X_mask,\n",
    "    Y_seq=Y, Y_mask=Y_mask)\n",
    "print(f'{out.size() = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 172\n"
     ]
    }
   ],
   "source": [
    "num_params = len(list(transformer.parameters()))\n",
    "\n",
    "print(f'Number of parameters: {num_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b166c11a6fb13fc284d60599e45a47824480cbed14934159809ec834d0d5166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
