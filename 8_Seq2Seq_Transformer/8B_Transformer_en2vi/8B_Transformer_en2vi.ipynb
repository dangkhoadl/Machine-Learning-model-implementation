{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50818\n",
      "['i thought you needed the sleep', 'you will survive.', 'he is a doctor and also a novelist', 'what you were taught is wrong.', \"i should've known tom would be here, too\"]\n"
     ]
    }
   ],
   "source": [
    "# Source seq\n",
    "with open('datasets/test/en', 'r+') as file_obj:\n",
    "    data_en = file_obj.readlines()\n",
    "data_en = [ line.strip().lower() for line in data_en ]\n",
    "\n",
    "print(len(data_en))\n",
    "print(data_en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50818\n",
      "['tôi nghĩ bạn cần ngủ', 'bạn sẽ sống sót.', 'ông là một bác sĩ và cũng là một tiểu thuyết gia', 'những gì bạn đã được dạy là sai.', 'tôi cũng nên biết tom cũng sẽ ở đây']\n"
     ]
    }
   ],
   "source": [
    "# Target seq\n",
    "with open('datasets/test/vi', 'r+') as file_obj:\n",
    "    data_vi = file_obj.readlines()\n",
    "data_vi = [ line.strip().lower() for line in data_vi ]\n",
    "\n",
    "print(len(data_vi))\n",
    "print(data_vi[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineerings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Source\n",
    "#### Source tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.special_tokens_map = {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "tokenizer.vocab_size = 30522\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 1045, 2245, 2017, 2734, 1996, 3637,  102,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2017, 2097, 5788, 1012,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2002, 2003, 1037, 3460, 1998, 2036, 1037, 9974,  102,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2054, 2017, 2020, 4036, 2003, 3308, 1012,  102,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 1045, 2323, 1005, 2310, 2124, 3419, 2052, 2022, 2182, 1010, 2205,\n",
      "          102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'{tokenizer.special_tokens_map = }')\n",
    "print(f'{tokenizer.vocab_size = }')\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text=data_en[:5],               # the batch sentences to be encoded\n",
    "    add_special_tokens=True,        # Add [CLS] and [SEP]\n",
    "    padding='longest',              # Add [PAD]s\n",
    "    return_attention_mask=True,     # Generate the attention mask\n",
    "    return_tensors='pt',            # ask the function to return PyTorch tensors\n",
    "    max_length=50,                  # maximum length of a sentence\n",
    "    truncation=True\n",
    ")\n",
    "pprint(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_en[0] = 'i thought you needed the sleep'\n",
      "encoded['input_ids'].size() = torch.Size([5, 13])\n",
      "ids = tensor([ 101, 1045, 2245, 2017, 2734, 1996, 3637,  102,    0,    0,    0,    0,\n",
      "           0])\n",
      "tokens = ['[CLS]', 'i', 'thought', 'you', 'needed', 'the', 'sleep', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "decoding = '[CLS] i thought you needed the sleep [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{data_en[0] = }\")\n",
    "print(f\"{encoded['input_ids'].size() = }\")\n",
    "\n",
    "ids = encoded['input_ids'][0]\n",
    "print(f'{ids = }')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(f'{tokens = }')\n",
    "\n",
    "decoding = tokenizer.decode(ids)\n",
    "print(f'{decoding = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Target\n",
    "#### Target Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "tokenizer.vocab_size = 64000\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[    0,    70,   487,    88,   115,   845,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,    88,    38,   235, 49850, 13208,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,    46,     8,    16,  2780,  9031,     6,    32,     8,    16,\n",
      "          3520, 12907,  3931,     2],\n",
      "        [    0,    21,   148,    88,    14,    11,   940,     8,  4101, 27375,\n",
      "             2,     1,     1,     1],\n",
      "        [    0,    70,    32,    77,    55, 33939,    32,    38,    25,    97,\n",
      "             2,     1,     1,     1]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "print(f'{tokenizer.special_tokens_map = }')\n",
    "print(f'{tokenizer.vocab_size = }')\n",
    "\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text=data_vi[:5],               # the batch sentences to be encoded\n",
    "    add_special_tokens=True,        # Add [CLS] and [SEP]\n",
    "    padding='longest',              # Add [PAD]s\n",
    "    return_attention_mask=True,     # Generate the attention mask\n",
    "    return_tensors='pt',            # ask the function to return PyTorch tensors\n",
    "    max_length=50,                  # maximum length of a sentence\n",
    "    truncation=True\n",
    ")\n",
    "pprint(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_vi[4] = 'tôi cũng nên biết tom cũng sẽ ở đây'\n",
      "encoded['input_ids'].size() = torch.Size([5, 14])\n",
      "ids = tensor([    0,    70,    32,    77,    55, 33939,    32,    38,    25,    97,\n",
      "            2,     1,     1,     1])\n",
      "tokens = ['<s>', 'tôi', 'cũng', 'nên', 'biết', 'tom', 'cũng', 'sẽ', 'ở', 'đây', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "decoding = '<s> tôi cũng nên biết tom cũng sẽ ở đây </s> <pad> <pad> <pad>'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{data_vi[4] = }\")\n",
    "print(f\"{encoded['input_ids'].size() = }\")\n",
    "\n",
    "ids = encoded['input_ids'][4]\n",
    "print(f'{ids = }')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(f'{tokens = }')\n",
    "\n",
    "decoding = tokenizer.decode(ids)\n",
    "print(f'{decoding = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vi_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx = 60, X_lexicon_size = {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Ty = 59, Y_lexicon_size = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "Tx = 60\n",
    "Ty = 59\n",
    "X_lexicon_size = en_tokenizer.special_tokens_map\n",
    "Y_lexicon_size = vi_tokenizer.special_tokens_map\n",
    "\n",
    "print(f'{Tx = }, {X_lexicon_size = }')\n",
    "print(f'{Ty = }, {Y_lexicon_size = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.Transformer import \\\n",
    "    create_mask_source, create_mask_target\n",
    "\n",
    "## Load model\n",
    "transformer = torch.load('ckpts/8B_Transformer_en2vi.model.pth')\n",
    "transformer = transformer.to('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(en_sentence, model,\n",
    "        en_tokenizer, vi_tokenizer,\n",
    "        Tx=60, Ty=59):\n",
    "    model = model.cpu()\n",
    "    model = model.eval()\n",
    "\n",
    "    # X\n",
    "    X_encoded = en_tokenizer(\n",
    "        text=en_sentence,               # the batch sentences to be encoded\n",
    "        add_special_tokens=True,        # Add [CLS] and [SEP]\n",
    "        padding='max_length',           # Add [PAD]s\n",
    "        return_attention_mask=True,     # Generate the attention mask\n",
    "        return_tensors='pt',            # ask the function to return PyTorch tensors\n",
    "        max_length=Tx,                  # maximum length of a sentence\n",
    "        truncation=True)\n",
    "    x_pad = en_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "    x_eos = en_tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "\n",
    "    ## (Tx)\n",
    "    X_seq = X_encoded['input_ids']\n",
    "    X_mask = create_mask_source(\n",
    "        X_seq=X_seq,\n",
    "        pad_token=x_pad,\n",
    "        device='cpu')\n",
    "    \n",
    "    # Debug\n",
    "    X_token = en_tokenizer.decode(X_seq.squeeze(dim=0))\n",
    "    print(f'{X_token = }')\n",
    "\n",
    "    # Init Y_seq = [<sos>] -> (1, Ty+1)\n",
    "    y_sos = vi_tokenizer.convert_tokens_to_ids('<s>')\n",
    "    y_pad = vi_tokenizer.convert_tokens_to_ids('<pad>')\n",
    "    Y_seq = torch.full((1, Ty+1), fill_value=y_pad, dtype=torch.int32)\n",
    "    Y_seq[0, 0] = y_sos\n",
    "\n",
    "    # t-1 -> t\n",
    "    for t in range(Ty):\n",
    "        # Infer\n",
    "        Y_mask = create_mask_target(\n",
    "            Y_seq=Y_seq[:,:-1],\n",
    "            pad_token=y_pad,\n",
    "            device='cpu')\n",
    "        with torch.no_grad():\n",
    "            yt_hat = model(\n",
    "                X_seq=X_seq, X_mask=X_mask,\n",
    "                Y_seq=Y_seq[:,:-1], Y_mask=Y_mask)\n",
    "        \n",
    "        # Debug\n",
    "        # y_pred = yt_hat.argmax(dim=-1)[0]\n",
    "        # y_pred_seq = vi_tokenizer.decode(y_pred)\n",
    "        # print(f'{Y_seq = }')\n",
    "\n",
    "        # Predict next token\n",
    "        best_guess = yt_hat.argmax(dim=-1)[:, t]\n",
    "\n",
    "        # Y_seq = (1, Ty)\n",
    "        Y_seq[:, t+1] = best_guess\n",
    "\n",
    "    # Output\n",
    "    Y_seq = Y_seq.squeeze(dim=0)\n",
    "    vi_sentence = vi_tokenizer.decode(Y_seq)\n",
    "    return vi_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_token = '[CLS] too much sugar for us. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(en_sentence=\"too much sugar for us.\",\n",
    "    model=transformer,\n",
    "    en_tokenizer=en_tokenizer, vi_tokenizer=vi_tokenizer,\n",
    "    Tx=Tx, Ty=Ty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b166c11a6fb13fc284d60599e45a47824480cbed14934159809ec834d0d5166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
